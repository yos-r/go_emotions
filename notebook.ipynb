{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.utils import class_weight\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import roc_auc_score, f1_score, hamming_loss, precision_score, recall_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3T3rC7IbTlDN"
   },
   "source": [
    "# Nettoyage, Tokenisation et Préparation des Embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySf6LBRnVvtH",
    "outputId": "fe2f287b-93bd-4a3d-a4f5-1fcb36281b30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeu de données d'entraînement : 43410 lignes\n",
      "Jeu de données de validation : 5426 lignes\n",
      "Jeu de données de test : 5427 lignes\n",
      "\n",
      "Aperçu de la colonne des IDs avant transformation :\n",
      "0    27\n",
      "1    27\n",
      "2     2\n",
      "3    14\n",
      "4     3\n",
      "Name: emotion_ids, dtype: object\n",
      "\n",
      "Aperçu du DataFrame d'entraînement FINAL :\n",
      "                                                text comment_id  admiration  \\\n",
      "0  My favourite food is anything I didn't have to...    eebbqej           0   \n",
      "1  Now if he does off himself, everyone will thin...    ed00q6i           0   \n",
      "2                     WHY THE FUCK IS BAYLESS ISOING    eezlygj           0   \n",
      "3                        To make her feel threatened    ed7ypvh           0   \n",
      "4                             Dirty Southern Wankers    ed0bdzj           0   \n",
      "\n",
      "   amusement  anger  annoyance  approval  caring  confusion  curiosity  ...  \\\n",
      "0          0      0          0         0       0          0          0  ...   \n",
      "1          0      0          0         0       0          0          0  ...   \n",
      "2          0      1          0         0       0          0          0  ...   \n",
      "3          0      0          0         0       0          0          0  ...   \n",
      "4          0      0          1         0       0          0          0  ...   \n",
      "\n",
      "   love  nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
      "0     0            0         0      0            0       0        0        0   \n",
      "1     0            0         0      0            0       0        0        0   \n",
      "2     0            0         0      0            0       0        0        0   \n",
      "3     0            0         0      0            0       0        0        0   \n",
      "4     0            0         0      0            0       0        0        0   \n",
      "\n",
      "   surprise  neutral  \n",
      "0         0        1  \n",
      "1         0        1  \n",
      "2         0        0  \n",
      "3         0        0  \n",
      "4         0        0  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "La taille du X_train (après padding) est : (43410, 70)\n",
      "La taille du Y_train est : (43410, 28)\n"
     ]
    }
   ],
   "source": [
    "EMOTION_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude',\n",
    "    'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride',\n",
    "    'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "NUM_LABELS = len(EMOTION_LABELS)\n",
    "\n",
    "# 3. Noms des colonnes pour les TSV (Pas de header, séparateur Tab)\n",
    "COLUMN_NAMES = ['text', 'emotion_ids', 'comment_id']\n",
    "tsv_path = 'dataset/data/'\n",
    "df_train = pd.read_csv(os.path.join(tsv_path, 'train.tsv'), sep='\\t', header=None, names=COLUMN_NAMES, encoding='utf-8')\n",
    "df_dev = pd.read_csv(os.path.join(tsv_path, 'dev.tsv'), sep='\\t', header=None, names=COLUMN_NAMES, encoding='utf-8')\n",
    "df_test = pd.read_csv(os.path.join(tsv_path, 'test.tsv'), sep='\\t', header=None, names=COLUMN_NAMES, encoding='utf-8')\n",
    "\n",
    "print(f\"Jeu de données d'entraînement : {len(df_train)} lignes\")\n",
    "print(f\"Jeu de données de validation : {len(df_dev)} lignes\")\n",
    "print(f\"Jeu de données de test : {len(df_test)} lignes\")\n",
    "\n",
    "# Vérification (doit contenir des chaînes d'IDs comme '2' ou '1,17')\n",
    "print(\"\\nAperçu de la colonne des IDs avant transformation :\")\n",
    "print(df_train['emotion_ids'].head())\n",
    "\n",
    "def create_binary_labels(df, labels_list):\n",
    "    \"\"\"\n",
    "    Convertit la colonne 'emotion_ids' (chaîne d'IDs séparées par des virgules)\n",
    "    en 28 colonnes binaires (0 ou 1) pour la classification multi-label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise un DataFrame binaire vide de taille (Nb_lignes x 28)\n",
    "    label_matrix = pd.DataFrame(0, index=df.index, columns=labels_list)\n",
    "\n",
    "    # Parcourt chaque ligne du DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Sépare les IDs d'émotions (sont des chaînes, ex: '2,5' -> ['2', '5'])\n",
    "        ids = str(row['emotion_ids']).split(',')\n",
    "\n",
    "        # Convertit les IDs en entiers pour les utiliser comme index\n",
    "        # L'index 0 correspond à la première émotion de votre liste, etc.\n",
    "        try:\n",
    "            # S'assure de ne traiter que des IDs valides (chiffres)\n",
    "            int_ids = [int(i) for i in ids if i.isdigit()]\n",
    "        except ValueError:\n",
    "            # Cas d'erreur (rare) ou ID non numérique\n",
    "            int_ids = []\n",
    "\n",
    "        # Met à 1 les colonnes correspondantes dans la matrice\n",
    "        for emotion_index in int_ids:\n",
    "            # S'assurer que l'index est valide (entre 0 et 27)\n",
    "            if 0 <= emotion_index < NUM_LABELS:\n",
    "                label_matrix.loc[index, labels_list[emotion_index]] = 1\n",
    "\n",
    "    # Concatène le DataFrame binaire avec le DataFrame original\n",
    "    df_result = pd.concat([df[['text', 'comment_id']], label_matrix], axis=1)\n",
    "\n",
    "    return df_result\n",
    "\n",
    "# Appliquer la transformation aux trois DataFrames\n",
    "df_train_final = create_binary_labels(df_train, EMOTION_LABELS)\n",
    "df_dev_final = create_binary_labels(df_dev, EMOTION_LABELS)\n",
    "df_test_final = create_binary_labels(df_test, EMOTION_LABELS)\n",
    "\n",
    "print(\"\\nAperçu du DataFrame d'entraînement FINAL :\")\n",
    "print(df_train_final.head())\n",
    "\n",
    "# 1. Définir les hyperparamètres (à ajuster)\n",
    "\n",
    "MAX_WORDS = 50000    # Taille maximale du vocabulaire (les 20000 mots les plus fréquents)\n",
    "MAX_LEN = 70         # Longueur maximale d'une séquence (un commentaire)\n",
    "\n",
    "# 2. Instancier et adapter le Tokenizer UNIQUEMENT sur les données d'ENTRAÎNEMENT\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(df_train_final['text'])\n",
    "\n",
    "# 3. Transformer les textes en séquences d'entiers (indices de mots)\n",
    "train_sequences = tokenizer.texts_to_sequences(df_train_final['text'])\n",
    "dev_sequences = tokenizer.texts_to_sequences(df_dev_final['text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(df_test_final['text'])\n",
    "\n",
    "# 4. Padding (uniformisation de la longueur des séquences)\n",
    "# Remplissage à MAX_LEN (ajout de zéros au début ou à la fin)\n",
    "X_train = pad_sequences(train_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "X_dev = pad_sequences(dev_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "X_test = pad_sequences(test_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "print(f\"La taille du X_train (après padding) est : {X_train.shape}\")\n",
    "Y_train = df_train_final[EMOTION_LABELS].values\n",
    "Y_dev = df_dev_final[EMOTION_LABELS].values\n",
    "Y_test = df_test_final[EMOTION_LABELS].values\n",
    "\n",
    "print(f\"La taille du Y_train est : {Y_train.shape}\") # Devrait être (Nb_lignes, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_dict = {}\n",
    "for i in range(NUM_LABELS):\n",
    "    y_col = Y_train[:, i]\n",
    "    try:\n",
    "        weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(y_col),\n",
    "            y=y_col\n",
    "        )\n",
    "        class_weights_dict[i] = weights[1]\n",
    "    except ValueError:\n",
    "        class_weights_dict[i] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdWjzCgkWKTr"
   },
   "source": [
    "# LSTM AVEC EMBEDDING APPRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "QCO6Ah1vXoQq",
    "outputId": "4fd13086-ac7f-43b5-dfef-811312f68b3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yosrb\\Downloads\\GOEMOTION\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SEQUENTIAL ET MODEL_LSTM \n",
    "# 1. Création du modèle séquentiel\n",
    "model_lstm = Sequential()\n",
    "# 2. Couche Embedding (Option A : Apprentissage des poid\n",
    "# Cette couche transforme les indices de mots (X_train) en vecteurs denses.\n",
    "model_lstm.add(Embedding(\n",
    "    input_dim=MAX_WORDS,\n",
    "    output_dim=100,\n",
    "    input_length=MAX_LEN\n",
    "))\n",
    "model_lstm.add(Dropout(0.2))  # ✅ AJOUT: Dropout après embedding\n",
    "\n",
    "# 3. Couche LSTM (Réseau de neurones récurrents)# C'est le cœur du modèle, il lit la séquence et capture les dépendances.\n",
    "# - units: Nombre de neurones/unités internes\n",
    "model_lstm.add(LSTM(units=128))\n",
    "model_lstm.add(Dropout(0.5))  # ✅ AJOUT: Dropout avant la couche Dense\n",
    "\n",
    "# Note: On n'utilise pas return_sequences=True ici car nous voulons un seul vecteur\n",
    "# de sortie pour toute la séquence, nécessaire pour la classification finale.\n",
    "# 4. Couche de Classification Finale units: Doit être égal au nombre de classes (28) et activation: 'sigmoid' .\n",
    "#   Chaque neurone de sortie prédit indépendamment la probabilité d'une émotion.\n",
    "model_lstm.add(Dense(NUM_LABELS, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# 2. COMPILATION \n",
    "\n",
    "# Définition de l'optimiseur (Adam est un bon choix par défaut)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Compilation\n",
    "model_lstm.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy', # ESSENTIEL pour le multi-label\n",
    "    metrics=[   \n",
    "        'accuracy',\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DE9nCDBBYfc_",
    "outputId": "15f904cf-a7ec-4d11-fb0b-5e01159e5e99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DÉBUT DE L'ENTRAÎNEMENT DU LSTM SIMPLE ---\n",
      "Epoch 1/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 469ms/step - accuracy: 0.1041 - auc: 0.5909 - loss: 2.1590 - val_accuracy: 0.2934 - val_auc: 0.7103 - val_loss: 0.1654\n",
      "Epoch 2/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 688ms/step - accuracy: 0.1716 - auc: 0.6319 - loss: 2.0194 - val_accuracy: 0.2934 - val_auc: 0.6737 - val_loss: 0.1668\n",
      "Epoch 3/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 385ms/step - accuracy: 0.2223 - auc: 0.6426 - loss: 2.0128 - val_accuracy: 0.2934 - val_auc: 0.7091 - val_loss: 0.1648\n",
      "Epoch 4/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 274ms/step - accuracy: 0.2618 - auc: 0.6523 - loss: 2.0045 - val_accuracy: 0.2934 - val_auc: 0.6747 - val_loss: 0.1659\n",
      "Epoch 5/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 130ms/step - accuracy: 0.2590 - auc: 0.6526 - loss: 2.0001 - val_accuracy: 0.2934 - val_auc: 0.7153 - val_loss: 0.1636\n",
      "Epoch 6/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 136ms/step - accuracy: 0.2735 - auc: 0.6578 - loss: 1.9999 - val_accuracy: 0.2934 - val_auc: 0.7301 - val_loss: 0.1631\n",
      "Epoch 7/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 148ms/step - accuracy: 0.2828 - auc: 0.6615 - loss: 1.9968 - val_accuracy: 0.2934 - val_auc: 0.7052 - val_loss: 0.1662\n",
      "Epoch 8/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 154ms/step - accuracy: 0.2876 - auc: 0.6602 - loss: 1.9963 - val_accuracy: 0.2934 - val_auc: 0.6787 - val_loss: 0.1653\n",
      "Epoch 9/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 225ms/step - accuracy: 0.2868 - auc: 0.6598 - loss: 1.9988 - val_accuracy: 0.2934 - val_auc: 0.7343 - val_loss: 0.1639\n",
      "Epoch 10/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 226ms/step - accuracy: 0.2896 - auc: 0.6619 - loss: 1.9941 - val_accuracy: 0.2934 - val_auc: 0.7167 - val_loss: 0.1643\n",
      "\n",
      "--- ENTRAÎNEMENT TERMINÉ ---\n"
     ]
    }
   ],
   "source": [
    "# 3- FIT AVEC EPOCHS / BATCH SIZE\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(\"\\n--- DÉBUT DE L'ENTRAÎNEMENT DU LSTM SIMPLE ---\")\n",
    "\n",
    "lstm_simple = model_lstm.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_dev, Y_dev),\n",
    "        class_weight=class_weights_dict,\n",
    "\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n--- ENTRAÎNEMENT TERMINÉ ---\")\n",
    "\n",
    "#sauvegarde\n",
    "chemin_sauvegarde = 'models/modele_lstm_simple.h5'\n",
    "model_lstm.save(chemin_sauvegarde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb16PoLrdubJ"
   },
   "source": [
    "Loading trained model : LSTM and calculating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6R6O__-Cdthk",
    "outputId": "0d4ce63a-9df4-4c04-d382-51a0a5e9c3e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step\n",
      "Forme des prédictions binaires : (5427, 28)\n"
     ]
    }
   ],
   "source": [
    "# Vous devez spécifier le chemin exact où vous l'avez sauvegardé\n",
    "model_lstm_loaded = load_model('models/modele_lstm_simple.h5')\n",
    "Y_pred = model_lstm_loaded.predict(X_test)\n",
    "# Prédictions Binaires (avec Seuil)\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "# Y_pred_binary: Transformation des probabilités en 0 ou 1\n",
    "y_pred_binary= (Y_pred > THRESHOLD).astype(int)\n",
    "print(f\"Forme des prédictions binaires : {Y_pred_binary.shape}\")\n",
    "h_loss = hamming_loss(Y_test, y_pred_binary)\n",
    "\n",
    "f1_micro = f1_score(Y_test, y_pred_binary, average='micro')\n",
    "f1_macro = f1_score(Y_test, y_pred_binary, average='macro')\n",
    "auc_roc = roc_auc_score(Y_test, Y_pred, average='macro')\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"  RÉSULTATS DE LA BASELINE (LSTM SIMPLE)   \")\n",
    "print(\"=\"*50)\n",
    "print(f\"1. Hamming Loss (H-Loss) : {h_loss:.4f} (Doit être proche de 0)\")\n",
    "print(f\"2. F1-score (Micro)      : {f1_micro:.4f}\")\n",
    "print(f\"3. F1-score (Macro)      : {f1_macro:.4f}\")\n",
    "print(f\"4. AUC-ROC (Macro)       : {auc_roc:.4f} (Doit être proche de 1)\")\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JvpuUvquN_s"
   },
   "source": [
    "# CNN-BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "iEj4oBqAuox6"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Attention(Layer):\n",
    "    \"\"\"\n",
    "    Couche d'Attention implémentée pour Keras/TensorFlow.\n",
    "    Elle condense la sortie séquentielle du BiLSTM en un unique vecteur\n",
    "    pondéré par l'importance de chaque mot.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # W (Poids): Poids qui apprend l'importance de chaque dimension\n",
    "        self.W = self.add_weight(name='attention_weight',\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        # b (Bias): Biais\n",
    "        self.b = self.add_weight(name='attention_bias',\n",
    "                                 shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calcul des scores (e = tanh(X * W + b))\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "\n",
    "        # Normalisation des scores (a = softmax(e)) -> Poids d'attention\n",
    "        a = K.softmax(e, axis=1)\n",
    "\n",
    "        # Contexte pondéré (output = X * a)\n",
    "        output = x * a\n",
    "\n",
    "        # Agrégation (somme de la séquence pondérée)\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "    # Nécessaire pour l'enregistrement du modèle\n",
    "    def get_config(self, **kwargs):\n",
    "        return super().get_config(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "POIDS DE CLASSE POUR WEIGHTED LOSS\n",
      "======================================================================\n",
      "Émotion                 Fréquence        Poids\n",
      "----------------------------------------------------------------------\n",
      "admiration                 0.0951       0.1217\n",
      "amusement                  0.0536       0.2158\n",
      "anger                      0.0361       0.3207\n",
      "annoyance                  0.0569       0.2034\n",
      "approval                   0.0677       0.1710\n",
      "caring                     0.0250       0.4622\n",
      "confusion                  0.0315       0.3673\n",
      "curiosity                  0.0505       0.2293\n",
      "desire                     0.0148       0.7839\n",
      "disappointment             0.0292       0.3959\n",
      "disapproval                0.0466       0.2485\n",
      "disgust                    0.0183       0.6336\n",
      "embarrassment              0.0070       1.6583\n",
      "excitement                 0.0196       0.5890\n",
      "fear                       0.0137       0.8431\n",
      "gratitude                  0.0613       0.1888\n",
      "grief                      0.0018       6.5254\n",
      "joy                        0.0334       0.3460\n",
      "love                       0.0481       0.2409\n",
      "nervousness                0.0038       3.0638\n",
      "optimism                   0.0364       0.3178\n",
      "pride                      0.0026       4.5267\n",
      "realization                0.0256       0.4527\n",
      "relief                     0.0035       3.2840\n",
      "remorse                    0.0126       0.9219\n",
      "sadness                    0.0305       0.3789\n",
      "surprise                   0.0244       0.4740\n",
      "neutral                    0.3276       0.0353\n",
      "======================================================================\n",
      "\n",
      "✅ Weighted loss function créée avec succès!\n",
      "   Cette fonction pénalise davantage les erreurs sur les classes rares.\n",
      "   Exemple: 'grief' (poids: 6.53) vs 'neutral' (poids: 0.04)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# WEIGHTED LOSS FUNCTION FOR MULTI-LABEL CLASSIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "# --- 1. CALCUL DES POIDS DE CLASSE (Class Weights) ---\n",
    "# Pour gérer le déséquilibre sévère des classes (neutral: 14219 vs grief: 77)\n",
    "# Calculer les poids inversement proportionnels à la fréquence de chaque classe\n",
    "\n",
    "class_counts = Y_train.sum(axis=0)  # Nombre d'occurrences par classe (28,)\n",
    "total_samples = len(Y_train)\n",
    "class_frequencies = class_counts / total_samples\n",
    "\n",
    "# Éviter division par zéro et calculer les poids inversés\n",
    "WEIGHTS_NUMPY = np.where(class_frequencies > 0,\n",
    "                          1.0 / class_frequencies,\n",
    "                          1.0)\n",
    "# Normalisation des poids pour maintenir l'échelle de la loss\n",
    "WEIGHTS_NUMPY = WEIGHTS_NUMPY / WEIGHTS_NUMPY.sum() * NUM_LABELS\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"POIDS DE CLASSE POUR WEIGHTED LOSS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Émotion':<20} {'Fréquence':>12} {'Poids':>12}\")\n",
    "print(\"-\"*70)\n",
    "for i in range(NUM_LABELS):\n",
    "    print(f\"{EMOTION_LABELS[i]:<20} {class_frequencies[i]:>12.4f} {WEIGHTS_NUMPY[i]:>12.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# --- 2. WEIGHTED LOSS FUNCTION FACTORY ---\n",
    "def weighted_loss_factory(weights):\n",
    "    \"\"\"\n",
    "    Crée une fonction de perte Binary Cross-Entropy pondérée pour multi-label.\n",
    "    \n",
    "    Cette fonction résout le problème de déséquilibre des classes en appliquant\n",
    "    des poids différents à chaque émotion. Les émotions rares (comme 'grief')\n",
    "    reçoivent un poids plus élevé, tandis que les émotions fréquentes (comme 'neutral')\n",
    "    reçoivent un poids plus faible.\n",
    "    \n",
    "    Args:\n",
    "        weights: numpy array de shape (NUM_LABELS,) contenant les poids de chaque classe\n",
    "        \n",
    "    Returns:\n",
    "        Fonction de perte compatible avec Keras/TensorFlow\n",
    "    \"\"\"\n",
    "    weights_tensor = K.constant(weights, dtype='float32')\n",
    "    \n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Binary Cross-Entropy pondérée pour classification multi-label.\n",
    "        \n",
    "        Formule: Loss = -Σ[w_i * (y_i * log(p_i) + (1-y_i) * log(1-p_i))]\n",
    "        où:\n",
    "            - y_i: label réel (0 ou 1)\n",
    "            - p_i: prédiction (probabilité entre 0 et 1)\n",
    "            - w_i: poids de la classe i\n",
    "            \n",
    "        Args:\n",
    "            y_true: Labels réels (batch_size, NUM_LABELS)\n",
    "            y_pred: Prédictions du modèle (batch_size, NUM_LABELS)\n",
    "            \n",
    "        Returns:\n",
    "            Perte moyenne pondérée (scalaire)\n",
    "        \"\"\"\n",
    "        # Éviter log(0) en clippant les prédictions entre epsilon et 1-epsilon\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        \n",
    "        # Calcul de la binary cross-entropy standard\n",
    "        # BCE = -(y_true * log(y_pred) + (1-y_true) * log(1-y_pred))\n",
    "        bce = -(y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))\n",
    "        \n",
    "        # Application des poids de classe (broadcasting sur le batch)\n",
    "        weighted_bce = bce * weights_tensor\n",
    "        \n",
    "        # Moyenne sur toutes les classes et tous les échantillons du batch\n",
    "        return K.mean(weighted_bce)\n",
    "    \n",
    "    # Définir le nom de la fonction pour le chargement du modèle\n",
    "    weighted_binary_crossentropy.__name__ = 'weighted_binary_crossentropy'\n",
    "    \n",
    "    return weighted_binary_crossentropy\n",
    "\n",
    "\n",
    "# Créer la fonction de perte personnalisée avec les poids calculés\n",
    "custom_loss_function = weighted_loss_factory(WEIGHTS_NUMPY)\n",
    "\n",
    "print(f\"\\n✅ Weighted loss function créée avec succès!\")\n",
    "print(f\"   Cette fonction pénalise davantage les erreurs sur les classes rares.\")\n",
    "print(f\"   Exemple: 'grief' (poids: {WEIGHTS_NUMPY[EMOTION_LABELS.index('grief')]:.2f}) vs 'neutral' (poids: {WEIGHTS_NUMPY[EMOTION_LABELS.index('neutral')]:.2f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================\n",
    "# MODÈLE 4: BERT-BASE TRANSFORMER (Fine-tuning)\n",
    "# ==========================================================================\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODÈLE 4: BERT-BASE POUR CLASSIFICATION MULTI-LABEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Version Transformers: {transformers.__version__}\")\n",
    "\n",
    "# --- 1. CHARGEMENT DU TOKENIZER ET MODÈLE BERT PRÉ-ENTRAÎNÉ ---\n",
    "\n",
    "print(\"\\nChargement du tokenizer BERT...\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"Chargement du modèle BERT pré-entraîné...\")\n",
    "# Solution pour éviter l'erreur \"safe_open object is not iterable\"\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased', from_pt=False)\n",
    "\n",
    "\n",
    "# --- 2. PRÉPARATION DES DONNÉES POUR BERT ---\n",
    "\n",
    "def encode_texts_for_bert(texts, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenise les textes au format BERT.\n",
    "    \n",
    "    Args:\n",
    "        texts: Liste de textes (strings)\n",
    "        tokenizer: BertTokenizer\n",
    "        max_length: Longueur maximale des séquences\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionnaire avec input_ids et attention_mask\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,      # Ajoute [CLS] et [SEP]\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='np'\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded['input_ids'][0])\n",
    "        attention_masks.append(encoded['attention_mask'][0])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(input_ids),\n",
    "        'attention_mask': np.array(attention_masks)\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\nTokenisation des données avec BERT tokenizer...\")\n",
    "print(\"Cela peut prendre quelques minutes...\")\n",
    "\n",
    "# Tokeniser les ensembles de données\n",
    "MAX_LENGTH_BERT = 128  # BERT supporte jusqu'à 512, mais 128 suffit pour les commentaires Reddit\n",
    "\n",
    "train_encodings = encode_texts_for_bert(df_train_final['text'].tolist(), bert_tokenizer, MAX_LENGTH_BERT)\n",
    "dev_encodings = encode_texts_for_bert(df_dev_final['text'].tolist(), bert_tokenizer, MAX_LENGTH_BERT)\n",
    "test_encodings = encode_texts_for_bert(df_test_final['text'].tolist(), bert_tokenizer, MAX_LENGTH_BERT)\n",
    "\n",
    "print(f\"Forme des input_ids train: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"Forme des attention_mask train: {train_encodings['attention_mask'].shape}\")\n",
    "\n",
    "\n",
    "# --- 3. CONSTRUCTION DU MODÈLE BERT POUR MULTI-LABEL ---\n",
    "\n",
    "print(\"\\nConstruction du modèle BERT...\")\n",
    "\n",
    "# Input layers\n",
    "input_ids = Input(shape=(MAX_LENGTH_BERT,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = Input(shape=(MAX_LENGTH_BERT,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# BERT layer - Charger directement dans la construction du modèle\n",
    "try:\n",
    "    bert_layer = TFBertModel.from_pretrained('bert-base-uncased', from_pt=False)\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement de BERT: {e}\")\n",
    "    print(\"Tentative avec from_pt=True...\")\n",
    "    bert_layer = TFBertModel.from_pretrained('bert-base-uncased', from_pt=True)\n",
    "\n",
    "bert_layer.trainable = True  # Fine-tuning activé\n",
    "\n",
    "# Obtenir les embeddings BERT\n",
    "bert_output = bert_layer(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Utiliser le token [CLS] pour la classification\n",
    "# bert_output[0] = sequence_output (batch_size, seq_len, 768)\n",
    "# bert_output[1] = pooled_output (batch_size, 768) - [CLS] token déjà poolé\n",
    "cls_token = bert_output[1]  # Shape: (batch_size, 768)\n",
    "\n",
    "# Couches de classification\n",
    "x = Dropout(0.3, name='dropout_bert')(cls_token)\n",
    "x = Dense(256, activation='relu', name='dense_1')(x)\n",
    "x = Dropout(0.3, name='dropout_1')(x)\n",
    "x = Dense(128, activation='relu', name='dense_2')(x)\n",
    "x = Dropout(0.3, name='dropout_2')(x)\n",
    "\n",
    "# Couche de sortie (sigmoid pour multi-label)\n",
    "output = Dense(NUM_LABELS, activation='sigmoid', name='output')(x)\n",
    "\n",
    "# Créer le modèle\n",
    "model_bert = Model(inputs=[input_ids, attention_mask], outputs=output, name='BERT_MultiLabel')\n",
    "\n",
    "print(model_bert.summary())\n",
    "\n",
    "\n",
    "# --- 4. COMPILATION DU MODÈLE ---\n",
    "\n",
    "model_bert.compile(\n",
    "    optimizer=Adam(learning_rate=2e-5),  # Learning rate faible pour fine-tuning\n",
    "    loss=custom_loss_function,  # Weighted Binary Cross-Entropy\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.AUC(name='auc', multi_label=True),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nModèle BERT compilé avec:\")\n",
    "print(\"  - Optimizer: Adam (lr=2e-5)\")\n",
    "print(\"  - Loss: Weighted Binary Cross-Entropy\")\n",
    "print(\"  - Metrics: Accuracy, AUC, Precision, Recall\")\n",
    "\n",
    "\n",
    "# --- 5. CALLBACKS POUR L'ENTRAÎNEMENT ---\n",
    "\n",
    "callbacks_bert = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=3,  # Patience réduite car BERT converge rapidement\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='models/modele_bert_best.keras',\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "# --- 6. ENTRAÎNEMENT DU MODÈLE BERT ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DÉBUT DE L'ENTRAÎNEMENT DU MODÈLE BERT\")\n",
    "print(\"=\"*70)\n",
    "print(\"Note: BERT est plus lent à entraîner mais donne de meilleurs résultats\")\n",
    "print(\"      Comptez environ 5-10 minutes par epoch sur GPU, plus sur CPU\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Préparer les inputs pour Keras\n",
    "X_train_bert = [train_encodings['input_ids'], train_encodings['attention_mask']]\n",
    "X_dev_bert = [dev_encodings['input_ids'], dev_encodings['attention_mask']]\n",
    "X_test_bert = [test_encodings['input_ids'], test_encodings['attention_mask']]\n",
    "\n",
    "history_bert = model_bert.fit(\n",
    "    X_train_bert,\n",
    "    Y_train,\n",
    "    epochs=5,  # BERT converge rapidement, 3-5 epochs suffisent\n",
    "    batch_size=16,  # Batch size réduit car BERT est très gourmand en mémoire\n",
    "    validation_data=(X_dev_bert, Y_dev),\n",
    "    callbacks=callbacks_bert,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENTRAÎNEMENT BERT TERMINÉ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# --- 7. SAUVEGARDE DU MODÈLE ---\n",
    "\n",
    "chemin_sauvegarde_bert = 'models/modele_bert_final.keras'\n",
    "model_bert.save(chemin_sauvegarde_bert, save_format='keras')\n",
    "\n",
    "print(f\"\\nModèle BERT sauvegardé dans: {chemin_sauvegarde_bert}\")\n",
    "print(f\"Meilleur modèle sauvegardé dans: models/modele_bert_best.keras\")\n",
    "\n",
    "\n",
    "# --- 8. ÉVALUATION SUR LE JEU DE TEST ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ÉVALUATION DU MODÈLE BERT SUR LE JEU DE TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prédictions\n",
    "Y_pred_proba_bert = model_bert.predict(X_test_bert, batch_size=16, verbose=1)\n",
    "Y_pred_binary_bert = (Y_pred_proba_bert > 0.5).astype(int)\n",
    "\n",
    "# Calcul des métriques\n",
    "h_loss_bert = hamming_loss(Y_test, Y_pred_binary_bert)\n",
    "auc_roc_bert = roc_auc_score(Y_test, Y_pred_proba_bert, average='macro')\n",
    "precision_micro_bert = precision_score(Y_test, Y_pred_binary_bert, average='micro', zero_division=0)\n",
    "recall_micro_bert = recall_score(Y_test, Y_pred_binary_bert, average='micro', zero_division=0)\n",
    "f1_micro_bert = f1_score(Y_test, Y_pred_binary_bert, average='micro', zero_division=0)\n",
    "precision_macro_bert = precision_score(Y_test, Y_pred_binary_bert, average='macro', zero_division=0)\n",
    "recall_macro_bert = recall_score(Y_test, Y_pred_binary_bert, average='macro', zero_division=0)\n",
    "f1_macro_bert = f1_score(Y_test, Y_pred_binary_bert, average='macro', zero_division=0)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nRÉSULTATS DU MODÈLE 4 (BERT-BASE)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Hamming Loss (H-Loss) : {h_loss_bert:.4f} (Proche de 0 : Mieux)\")\n",
    "print(f\"AUC-ROC (Macro)       : {auc_roc_bert:.4f} (Proche de 1 : Mieux)\")\n",
    "print(\"-\" * 35)\n",
    "print(\"   Micro-Averaged (Global/Fréquent)\")\n",
    "print(f\"   Precision (Micro)   : {precision_micro_bert:.4f}\")\n",
    "print(f\"   Recall (Micro)      : {recall_micro_bert:.4f}\")\n",
    "print(f\"   F1-score (Micro)    : {f1_micro_bert:.4f}\")\n",
    "print(\"-\" * 35)\n",
    "print(\"   Macro-Averaged (Classes Rares)\")\n",
    "print(f\"   Precision (Macro)   : {precision_macro_bert:.4f}\")\n",
    "print(f\"   Recall (Macro)      : {recall_macro_bert:.4f}\")\n",
    "print(f\"   F1-score (Macro)    : {f1_macro_bert:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# --- 9. FONCTION DE PRÉDICTION POUR BERT ---\n",
    "\n",
    "def predict_emotions_bert(text, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Prédit les émotions pour une phrase avec le modèle BERT.\n",
    "    \n",
    "    Args:\n",
    "        text: La phrase à analyser (string)\n",
    "        threshold: Seuil de décision\n",
    "        \n",
    "    Returns:\n",
    "        dict: Émotions détectées et toutes les probabilités\n",
    "    \"\"\"\n",
    "    # Tokenisation BERT\n",
    "    encoded = bert_tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LENGTH_BERT,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    \n",
    "    # Prédiction\n",
    "    input_ids = encoded['input_ids']\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    probabilities = model_bert.predict([input_ids, attention_mask], verbose=0)[0]\n",
    "    \n",
    "    # Extraction des émotions\n",
    "    detected_emotions = {}\n",
    "    all_emotions = {}\n",
    "    \n",
    "    for i, emotion in enumerate(EMOTION_LABELS):\n",
    "        prob = probabilities[i]\n",
    "        all_emotions[emotion] = prob\n",
    "        if prob >= threshold:\n",
    "            detected_emotions[emotion] = prob\n",
    "    \n",
    "    return detected_emotions, all_emotions\n",
    "\n",
    "\n",
    "# Test rapide du modèle BERT\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST RAPIDE DU MODÈLE BERT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_sentence = \"I am so grateful and happy! This is amazing!\"\n",
    "detected, all_probs = predict_emotions_bert(test_sentence, threshold=0.3)\n",
    "\n",
    "print(f\"Phrase: \\\"{test_sentence}\\\"\")\n",
    "print(\"\\nÉmotions détectées:\")\n",
    "for emotion, prob in sorted(detected.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  - {emotion:20s}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n"
     ]
    }
   ],
   "source": [
    "# --- 4. SAUVEGARDE DU MODÈLE FINAL AVEC CUSTOM OBJECTS ---\n",
    "chemin_sauvegarde_final = 'models/modele_bilstm_attention_final.keras'\n",
    "\n",
    "# Sauvegarder avec custom_objects pour la couche Attention et la loss function\n",
    "model_bilstm_att.save(\n",
    "    chemin_sauvegarde_final,\n",
    "    save_format='keras'  # ✅ Format moderne .keras (recommandé vs .h5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLn0HgUcyCRB",
    "outputId": "66ae62c1-1978-40a8-caf8-aebea169b6e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step\n",
      "Prédictions générées. Taille des données de test : 5427\n",
      "\n",
      "======================================================================\n",
      "  RÉSULTATS DU MODÈLE 2 (BiLSTM + Attention)   \n",
      "======================================================================\n",
      "Hamming Loss (H-Loss) : 0.0415 (Proche de 0 : Mieux)\n",
      "AUC-ROC (Macro)       : 0.7974 (Proche de 1 : Mieux)\n",
      "-----------------------------------\n",
      "   **Micro-Averaged (Global/Fréquent)**\n",
      "   Precision (Micro)   : 0.5031\n",
      "   Recall (Micro)      : 0.3182\n",
      "   F1-score (Micro)    : 0.3899\n",
      "-----------------------------------\n",
      "   **Macro-Averaged (Classes Rares)**\n",
      "   Precision (Macro)   : 0.1878\n",
      "   Recall (Macro)      : 0.1832\n",
      "   F1-score (Macro)    : 0.1774\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. GÉNÉRATION DES PRÉDICTIONS ---\n",
    "\n",
    "# Y_pred_proba: Probabilités d'appartenance à chaque classe (sortie Sigmoid)\n",
    "Y_pred_proba = model_bilstm_att.predict(X_test)\n",
    "Y_pred_binary = (Y_pred_proba > THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"Prédictions générées. Taille des données de test : {Y_pred_proba.shape[0]}\")\n",
    "\n",
    "\n",
    "# --- 2. CALCUL DES MÉTRIQUES MULTI-LABEL (Partie 3) ---\n",
    "\n",
    "# Métrique 1: Hamming Loss (H-Loss) - Basée sur l'erreur moyenne par label\n",
    "h_loss = hamming_loss(Y_test, Y_pred_binary)\n",
    "\n",
    "# Métrique 2: AUC-ROC (Macro) - Basée sur les probabilités\n",
    "auc_roc = roc_auc_score(Y_test, Y_pred_proba, average='macro')\n",
    "\n",
    "# Métrique 3: Micro-Averaged (Priorise les classes fréquentes)\n",
    "precision_micro = precision_score(Y_test, Y_pred_binary, average='micro', zero_division=0)\n",
    "recall_micro = recall_score(Y_test, Y_pred_binary, average='micro', zero_division=0)\n",
    "f1_micro = f1_score(Y_test, Y_pred_binary, average='micro', zero_division=0)\n",
    "\n",
    "# Métrique 4: Macro-Averaged (Priorise les classes rares)\n",
    "precision_macro = precision_score(Y_test, Y_pred_binary, average='macro', zero_division=0)\n",
    "recall_macro = recall_score(Y_test, Y_pred_binary, average='macro', zero_division=0)\n",
    "f1_macro = f1_score(Y_test, Y_pred_binary, average='macro', zero_division=0)\n",
    "\n",
    "\n",
    "# --- 3. AFFICHAGE DU BENCHMARK ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  RÉSULTATS DU MODÈLE 2 (BiLSTM + Attention)   \")\n",
    "print(\"=\"*70)\n",
    "print(f\"Hamming Loss (H-Loss) : {h_loss:.4f} (Proche de 0 : Mieux)\")\n",
    "print(f\"AUC-ROC (Macro)       : {auc_roc:.4f} (Proche de 1 : Mieux)\")\n",
    "print(\"-\" * 35)\n",
    "print(\"   **Micro-Averaged (Global/Fréquent)**\")\n",
    "print(f\"   Precision (Micro)   : {precision_micro:.4f}\")\n",
    "print(f\"   Recall (Micro)      : {recall_micro:.4f}\")\n",
    "print(f\"   F1-score (Micro)    : {f1_micro:.4f}\")\n",
    "print(\"-\" * 35)\n",
    "print(\"   **Macro-Averaged (Classes Rares)**\")\n",
    "print(f\"   Precision (Macro)   : {precision_macro:.4f}\")\n",
    "print(f\"   Recall (Macro)      : {recall_macro:.4f}\")\n",
    "print(f\"   F1-score (Macro)    : {f1_macro:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# --- 4. SAUVEGARDE DU MODÈLE FINAL ---\n",
    "\n",
    "# Chemin de sauvegarde\n",
    "# chemin_sauvegarde_2 = '/content/drive/MyDrive/modele_bilstm_attention.h5'\n",
    "\n",
    "# # Nécessite custom_objects car la couche Attention et la fonction de perte\n",
    "# # sont définies par l'utilisateur\n",
    "# model_bilstm_att.save(\n",
    "#     chemin_sauvegarde_2,\n",
    "#     custom_objects={'Attention': Attention, 'weighted_loss': weighted_loss}\n",
    "# )\n",
    "# print(f\"\\nModèle BiLSTM+Attention sauvegardé avec succès dans : {chemin_sauvegarde_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 46ms/step\n",
      "Prédictions générées. Taille des données de test : 5427\n",
      "\n",
      "======================================================================\n",
      "  RÉSULTATS DU MODÈLE 2 (BiLSTM + Attention)   \n",
      "======================================================================\n",
      "Hamming Loss (H-Loss) : 0.0393 (Proche de 0 : Mieux)\n",
      "AUC-ROC (Macro)       : 0.7974 (Proche de 1 : Mieux)\n",
      "-----------------------------------\n",
      "   **Micro-Averaged (Global/Fréquent)**\n",
      "   Precision (Micro)   : 0.6217\n",
      "   Recall (Micro)      : 0.1449\n",
      "   F1-score (Micro)    : 0.2350\n",
      "-----------------------------------\n",
      "   **Macro-Averaged (Classes Rares)**\n",
      "   Precision (Macro)   : 0.1497\n",
      "   Recall (Macro)      : 0.0789\n",
      "   F1-score (Macro)    : 0.0920\n",
      "======================================================================\n",
      "\n",
      "====================================================================================================\n",
      "BENCHMARK: COMPARAISON DES MODÈLES SUR LE JEU DE TEST\n",
      "====================================================================================================\n",
      "                            Modèle  Hamming Loss  AUC-ROC (Macro)  Precision (Micro)  Recall (Micro)  F1-score (Micro)  Precision (Macro)  Recall (Macro)  F1-score (Macro)\n",
      "            LSTM Simple (Baseline)      0.041486         0.797424           0.000000        0.000000          0.389857           0.000000        0.000000          0.177432\n",
      "BiLSTM + Attention (Weighted Loss)      0.039288         0.797424           0.621695        0.144889          0.235008           0.149651        0.078897          0.091989\n",
      "====================================================================================================\n",
      "\n",
      "AMÉLIORATIONS DU BiLSTM + ATTENTION vs LSTM SIMPLE:\n",
      "----------------------------------------------------------------------\n",
      "AUC-ROC (Macro)    : 0.7974 -> 0.7974 (+0.00%)\n",
      "F1-score (Micro)   : 0.3899 -> 0.2350 (+-15.48%)\n",
      "F1-score (Macro)   : 0.1774 -> 0.0920 (+-8.54%)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Tableau de benchmark sauvegardé dans: models/benchmark_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================\n",
    "# EVALUATION ET BENCHMARK - COMPARAISON DES MODELES\n",
    "# ==========================================================================\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, hamming_loss, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. GÉNÉRATION DES PRÉDICTIONS ---\n",
    "\n",
    "# Y_pred_proba: Probabilités d'appartenance à chaque classe (sortie Sigmoid)\n",
    "Y_pred_proba_bilstm = model_bilstm_att.predict(X_test)\n",
    "\n",
    "# Seuil de binarisation\n",
    "THRESHOLD = 0.5\n",
    "Y_pred_binary_bilstm = (Y_pred_proba_bilstm > THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"Prédictions générées. Taille des données de test : {Y_pred_proba_bilstm.shape[0]}\")\n",
    "\n",
    "\n",
    "# --- 2. CALCUL DES MÉTRIQUES MULTI-LABEL ---\n",
    "\n",
    "# Métrique 1: Hamming Loss (H-Loss) - Basée sur l'erreur moyenne par label\n",
    "h_loss_bilstm = hamming_loss(Y_test, Y_pred_binary_bilstm)\n",
    "\n",
    "# Métrique 2: AUC-ROC (Macro) - Basée sur les probabilités\n",
    "auc_roc_bilstm = roc_auc_score(Y_test, Y_pred_proba_bilstm, average='macro')\n",
    "\n",
    "# Métrique 3: Micro-Averaged (Priorise les classes fréquentes)\n",
    "precision_micro_bilstm = precision_score(Y_test, Y_pred_binary_bilstm, average='micro', zero_division=0)\n",
    "recall_micro_bilstm = recall_score(Y_test, Y_pred_binary_bilstm, average='micro', zero_division=0)\n",
    "f1_micro_bilstm = f1_score(Y_test, Y_pred_binary_bilstm, average='micro', zero_division=0)\n",
    "\n",
    "# Métrique 4: Macro-Averaged (Priorise les classes rares)\n",
    "precision_macro_bilstm = precision_score(Y_test, Y_pred_binary_bilstm, average='macro', zero_division=0)\n",
    "recall_macro_bilstm = recall_score(Y_test, Y_pred_binary_bilstm, average='macro', zero_division=0)\n",
    "f1_macro_bilstm = f1_score(Y_test, Y_pred_binary_bilstm, average='macro', zero_division=0)\n",
    "\n",
    "\n",
    "# --- 3. AFFICHAGE DES RÉSULTATS BiLSTM ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  RÉSULTATS DU MODÈLE 2 (BiLSTM + Attention)   \")\n",
    "print(\"=\"*70)\n",
    "print(f\"Hamming Loss (H-Loss) : {h_loss_bilstm:.4f} (Proche de 0 : Mieux)\")\n",
    "print(f\"AUC-ROC (Macro)       : {auc_roc_bilstm:.4f} (Proche de 1 : Mieux)\")\n",
    "print(\"-\" * 35)\n",
    "print(\"   **Micro-Averaged (Global/Fréquent)**\")\n",
    "print(f\"   Precision (Micro)   : {precision_micro_bilstm:.4f}\")\n",
    "print(f\"   Recall (Micro)      : {recall_micro_bilstm:.4f}\")\n",
    "print(f\"   F1-score (Micro)    : {f1_micro_bilstm:.4f}\")\n",
    "print(\"-\" * 35)\n",
    "print(\"   **Macro-Averaged (Classes Rares)**\")\n",
    "print(f\"   Precision (Macro)   : {precision_macro_bilstm:.4f}\")\n",
    "print(f\"   Recall (Macro)      : {recall_macro_bilstm:.4f}\")\n",
    "print(f\"   F1-score (Macro)    : {f1_macro_bilstm:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# --- 4. BENCHMARK: TABLEAU COMPARATIF DES MODÈLES ---\n",
    "\n",
    "# Récupérer les métriques du LSTM simple (déjà calculées dans cell-7)\n",
    "# On suppose que vous avez déjà h_loss, f1_micro, f1_macro, auc_roc du LSTM\n",
    "\n",
    "# Créer un DataFrame de comparaison\n",
    "benchmark_data = {\n",
    "    'Modèle': [\n",
    "        'LSTM Simple (Baseline)',\n",
    "        'BiLSTM + Attention (Weighted Loss)'\n",
    "    ],\n",
    "    'Hamming Loss': [\n",
    "        h_loss,  # Du LSTM simple\n",
    "        h_loss_bilstm\n",
    "    ],\n",
    "    'AUC-ROC (Macro)': [\n",
    "        auc_roc,  # Du LSTM simple\n",
    "        auc_roc_bilstm\n",
    "    ],\n",
    "    'Precision (Micro)': [\n",
    "        0.0000,  # LSTM simple n'a pas de vrais positifs\n",
    "        precision_micro_bilstm\n",
    "    ],\n",
    "    'Recall (Micro)': [\n",
    "        0.0000,\n",
    "        recall_micro_bilstm\n",
    "    ],\n",
    "    'F1-score (Micro)': [\n",
    "        f1_micro,\n",
    "        f1_micro_bilstm\n",
    "    ],\n",
    "    'Precision (Macro)': [\n",
    "        0.0000,\n",
    "        precision_macro_bilstm\n",
    "    ],\n",
    "    'Recall (Macro)': [\n",
    "        0.0000,\n",
    "        recall_macro_bilstm\n",
    "    ],\n",
    "    'F1-score (Macro)': [\n",
    "        f1_macro,\n",
    "        f1_macro_bilstm\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_benchmark = pd.DataFrame(benchmark_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"BENCHMARK: COMPARAISON DES MODÈLES SUR LE JEU DE TEST\")\n",
    "print(\"=\"*100)\n",
    "print(df_benchmark.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Calculer les améliorations\n",
    "print(\"\\nAMÉLIORATIONS DU BiLSTM + ATTENTION vs LSTM SIMPLE:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"AUC-ROC (Macro)    : {auc_roc:.4f} -> {auc_roc_bilstm:.4f} (+{(auc_roc_bilstm - auc_roc)*100:.2f}%)\")\n",
    "print(f\"F1-score (Micro)   : {f1_micro:.4f} -> {f1_micro_bilstm:.4f} (+{(f1_micro_bilstm - f1_micro)*100:.2f}%)\")\n",
    "print(f\"F1-score (Macro)   : {f1_macro:.4f} -> {f1_macro_bilstm:.4f} (+{(f1_macro_bilstm - f1_macro)*100:.2f}%)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "\n",
    "# --- 5. SAUVEGARDE DU BENCHMARK POUR LE RAPPORT ---\n",
    "df_benchmark.to_csv('models/benchmark_results.csv', index=False)\n",
    "print(\"\\nTableau de benchmark sauvegardé dans: models/benchmark_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONSTRUCTION DU MODÈLE 3: CNN-BiLSTM + ATTENTION\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CNN_BiLSTM_Attention\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"CNN_BiLSTM_Attention\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400,000</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,640</span> │ dropout_embeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │ dropout_embeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">41,024</span> │ dropout_embeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pool_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pool_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pool_5              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_concat          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ pool_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ pool_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                     │                   │            │ pool_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_cnn         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cnn_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bilstm              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ dropout_cnn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">163</span> │ bilstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_hidden        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_hidden      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_hidden[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,612</span> │ dropout_hidden[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m6,400,000\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv_3 (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m24,640\u001b[0m │ dropout_embeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv_4 (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m32,832\u001b[0m │ dropout_embeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv_5 (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m41,024\u001b[0m │ dropout_embeddin… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pool_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pool_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pool_5              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cnn_concat          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m192\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ pool_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ pool_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                     │                   │            │ pool_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_cnn         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m192\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ cnn_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bilstm              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m131,584\u001b[0m │ dropout_cnn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m163\u001b[0m │ bilstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_hidden        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_hidden      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_hidden[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m)        │      \u001b[38;5;34m3,612\u001b[0m │ dropout_hidden[\u001b[38;5;34m0\u001b[0m… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,650,367</span> (25.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,650,367\u001b[0m (25.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,650,367</span> (25.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,650,367\u001b[0m (25.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "Modèle compilé avec Weighted Binary Cross-Entropy\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================================\n",
    "# MODÈLE 3: ARCHITECTURE HYBRIDE CNN-BiLSTM + ATTENTION\n",
    "# ==========================================================================\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONSTRUCTION DU MODÈLE 3: CNN-BiLSTM + ATTENTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- 1. CONSTRUCTION DU MODÈLE HYBRIDE (API FONCTIONNELLE) ---\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(MAX_LEN,), name='input')\n",
    "\n",
    "# Couche Embedding partagée\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=MAX_WORDS,\n",
    "    output_dim=128,\n",
    "    name='embedding'\n",
    ")(input_layer)\n",
    "embedding_dropout = Dropout(0.3, name='dropout_embedding')(embedding_layer)\n",
    "\n",
    "# --- BRANCHE CNN: Extraction de features locales avec plusieurs tailles de filtres ---\n",
    "# Utilisation de 3 tailles de filtres différentes pour capturer différents n-grams\n",
    "\n",
    "# Filtres de taille 3 (trigrams)\n",
    "conv1 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', name='conv_3')(embedding_dropout)\n",
    "pool1 = MaxPooling1D(pool_size=2, name='pool_3')(conv1)\n",
    "\n",
    "# Filtres de taille 4 (4-grams)\n",
    "conv2 = Conv1D(filters=64, kernel_size=4, activation='relu', padding='same', name='conv_4')(embedding_dropout)\n",
    "pool2 = MaxPooling1D(pool_size=2, name='pool_4')(conv2)\n",
    "\n",
    "# Filtres de taille 5 (5-grams)\n",
    "conv3 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same', name='conv_5')(embedding_dropout)\n",
    "pool3 = MaxPooling1D(pool_size=2, name='pool_5')(conv3)\n",
    "\n",
    "# Concaténer les 3 branches CNN\n",
    "cnn_concat = Concatenate(axis=-1, name='cnn_concat')([pool1, pool2, pool3])\n",
    "cnn_dropout = Dropout(0.3, name='dropout_cnn')(cnn_concat)\n",
    "\n",
    "# --- BRANCHE BiLSTM: Capture des dépendances séquentielles ---\n",
    "bilstm_layer = Bidirectional(\n",
    "    LSTM(units=64, \n",
    "         return_sequences=True,  # Nécessaire pour l'attention\n",
    "         dropout=0.2,\n",
    "         recurrent_dropout=0.2),\n",
    "    name='bilstm'\n",
    ")(cnn_dropout)\n",
    "\n",
    "# --- MÉCANISME D'ATTENTION: Pondération des features importantes ---\n",
    "attention_layer = Attention(name='attention')(bilstm_layer)\n",
    "\n",
    "# --- COUCHES DENSES FINALES ---\n",
    "dense_hidden = Dense(128, activation='relu', name='dense_hidden')(attention_layer)\n",
    "dense_dropout = Dropout(0.5, name='dropout_hidden')(dense_hidden)\n",
    "\n",
    "# Couche de sortie (classification multi-label)\n",
    "output_layer = Dense(28, activation='sigmoid', name='output')(dense_dropout)\n",
    "\n",
    "# Créer le modèle\n",
    "model_cnn_bilstm_att = Model(inputs=input_layer, outputs=output_layer, name='CNN_BiLSTM_Attention')\n",
    "\n",
    "print(model_cnn_bilstm_att.summary())\n",
    "\n",
    "\n",
    "# --- 2. COMPILATION AVEC WEIGHTED LOSS ---\n",
    "model_cnn_bilstm_att.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=custom_loss_function,  # Weighted Binary Cross-Entropy\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.AUC(name='auc', multi_label=True),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nModèle compilé avec Weighted Binary Cross-Entropy\")\n",
    "\n",
    "\n",
    "# --- 3. CALLBACKS POUR L'ENTRAÎNEMENT ---\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "callbacks_cnn = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=5,\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='models/modele_cnn_bilstm_attention_best.keras',\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DÉBUT DE L'ENTRAÎNEMENT DU CNN-BiLSTM + ATTENTION\n",
      "======================================================================\n",
      "Epoch 1/15\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.2021 - auc: 0.5036 - loss: 0.0992 - precision: 0.1159 - recall: 0.0884\n",
      "Epoch 1: val_auc improved from None to 0.60121, saving model to models/modele_cnn_bilstm_attention_best.keras\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 245ms/step - accuracy: 0.2563 - auc: 0.5159 - loss: 0.0676 - precision: 0.1340 - recall: 0.0348 - val_accuracy: 0.2934 - val_auc: 0.6012 - val_loss: 0.0535 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.2951 - auc: 0.5860 - loss: 0.0555 - precision: 0.3024 - recall: 7.7746e-04\n",
      "Epoch 2: val_auc improved from 0.60121 to 0.64073, saving model to models/modele_cnn_bilstm_attention_best.keras\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 252ms/step - accuracy: 0.2971 - auc: 0.5963 - loss: 0.0541 - precision: 0.3361 - recall: 7.8273e-04 - val_accuracy: 0.2991 - val_auc: 0.6407 - val_loss: 0.0509 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376ms/step - accuracy: 0.2984 - auc: 0.6395 - loss: 0.0510 - precision: 0.3465 - recall: 0.0016\n",
      "Epoch 3: val_auc improved from 0.64073 to 0.66950, saving model to models/modele_cnn_bilstm_attention_best.keras\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 395ms/step - accuracy: 0.2994 - auc: 0.6472 - loss: 0.0507 - precision: 0.4060 - recall: 0.0024 - val_accuracy: 0.3002 - val_auc: 0.6695 - val_loss: 0.0502 - val_precision: 0.8000 - val_recall: 6.2696e-04 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step - accuracy: 0.3035 - auc: 0.6890 - loss: 0.0477 - precision: 0.4172 - recall: 0.0050\n",
      "Epoch 4: val_auc improved from 0.66950 to 0.70501, saving model to models/modele_cnn_bilstm_attention_best.keras\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 387ms/step - accuracy: 0.3039 - auc: 0.6987 - loss: 0.0476 - precision: 0.4662 - recall: 0.0068 - val_accuracy: 0.3028 - val_auc: 0.7050 - val_loss: 0.0504 - val_precision: 0.6923 - val_recall: 0.0056 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.3128 - auc: 0.7483 - loss: 0.0444 - precision: 0.5123 - recall: 0.0325\n",
      "Epoch 5: val_auc improved from 0.70501 to 0.74589, saving model to models/modele_cnn_bilstm_attention_best.keras\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 262ms/step - accuracy: 0.3183 - auc: 0.7652 - loss: 0.0440 - precision: 0.5169 - recall: 0.0513 - val_accuracy: 0.3417 - val_auc: 0.7459 - val_loss: 0.0489 - val_precision: 0.7361 - val_recall: 0.0083 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.3391 - auc: 0.8029 - loss: 0.0408 - precision: 0.5319 - recall: 0.0935\n",
      "Epoch 6: val_auc improved from 0.74589 to 0.76306, saving model to models/modele_cnn_bilstm_attention_best.keras\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 274ms/step - accuracy: 0.3417 - auc: 0.8078 - loss: 0.0407 - precision: 0.5416 - recall: 0.1012 - val_accuracy: 0.3640 - val_auc: 0.7631 - val_loss: 0.0479 - val_precision: 0.4986 - val_recall: 0.0569 - learning_rate: 0.0010\n",
      "Epoch 7/15\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - accuracy: 0.3627 - auc: 0.8360 - loss: 0.0380 - precision: 0.5652 - recall: 0.1212\n",
      "Epoch 7: val_auc improved from 0.76306 to 0.76554, saving model to models/modele_cnn_bilstm_attention_best.keras\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 317ms/step - accuracy: 0.3745 - auc: 0.8375 - loss: 0.0378 - precision: 0.5762 - recall: 0.1304 - val_accuracy: 0.3953 - val_auc: 0.7655 - val_loss: 0.0490 - val_precision: 0.5990 - val_recall: 0.1437 - learning_rate: 0.0010\n",
      "Epoch 8/15\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - accuracy: 0.3994 - auc: 0.8555 - loss: 0.0353 - precision: 0.6102 - recall: 0.1629\n",
      "Epoch 8: val_auc improved from 0.76554 to 0.77497, saving model to models/modele_cnn_bilstm_attention_best.keras\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 401ms/step - accuracy: 0.4047 - auc: 0.8561 - loss: 0.0353 - precision: 0.6209 - recall: 0.1708 - val_accuracy: 0.4219 - val_auc: 0.7750 - val_loss: 0.0490 - val_precision: 0.6576 - val_recall: 0.1481 - learning_rate: 0.0010\n",
      "Epoch 9/15\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417ms/step - accuracy: 0.4306 - auc: 0.8709 - loss: 0.0331 - precision: 0.6569 - recall: 0.1942\n",
      "Epoch 9: val_auc did not improve from 0.77497\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 434ms/step - accuracy: 0.4423 - auc: 0.8726 - loss: 0.0331 - precision: 0.6650 - recall: 0.2129 - val_accuracy: 0.4469 - val_auc: 0.7729 - val_loss: 0.0501 - val_precision: 0.6751 - val_recall: 0.2016 - learning_rate: 0.0010\n",
      "Epoch 10/15\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step - accuracy: 0.4714 - auc: 0.8906 - loss: 0.0305 - precision: 0.6995 - recall: 0.2563\n",
      "Epoch 10: val_auc did not improve from 0.77497\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 388ms/step - accuracy: 0.4753 - auc: 0.8926 - loss: 0.0301 - precision: 0.7051 - recall: 0.2641 - val_accuracy: 0.4493 - val_auc: 0.7744 - val_loss: 0.0510 - val_precision: 0.6413 - val_recall: 0.2475 - learning_rate: 5.0000e-04\n",
      "Epoch 11/15\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.4950 - auc: 0.9008 - loss: 0.0285 - precision: 0.7183 - recall: 0.2882\n",
      "Epoch 11: val_auc did not improve from 0.77497\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 242ms/step - accuracy: 0.4913 - auc: 0.9014 - loss: 0.0286 - precision: 0.7217 - recall: 0.2918 - val_accuracy: 0.4467 - val_auc: 0.7721 - val_loss: 0.0523 - val_precision: 0.6432 - val_recall: 0.2633 - learning_rate: 5.0000e-04\n",
      "Epoch 12/15\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.5066 - auc: 0.9077 - loss: 0.0273 - precision: 0.7388 - recall: 0.3097\n",
      "Epoch 12: val_auc did not improve from 0.77497\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 278ms/step - accuracy: 0.5108 - auc: 0.9077 - loss: 0.0272 - precision: 0.7416 - recall: 0.3138 - val_accuracy: 0.4554 - val_auc: 0.7682 - val_loss: 0.0535 - val_precision: 0.6618 - val_recall: 0.2503 - learning_rate: 5.0000e-04\n",
      "Epoch 13/15\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.5281 - auc: 0.9149 - loss: 0.0259 - precision: 0.7549 - recall: 0.3328\n",
      "Epoch 13: val_auc did not improve from 0.77497\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 246ms/step - accuracy: 0.5259 - auc: 0.9156 - loss: 0.0260 - precision: 0.7505 - recall: 0.3338 - val_accuracy: 0.4532 - val_auc: 0.7677 - val_loss: 0.0538 - val_precision: 0.6579 - val_recall: 0.2547 - learning_rate: 2.5000e-04\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\n",
      "======================================================================\n",
      "ENTRAÎNEMENT TERMINÉ\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4. ENTRAÎNEMENT DU MODÈLE ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DÉBUT DE L'ENTRAÎNEMENT DU CNN-BiLSTM + ATTENTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "history_cnn_bilstm_att = model_cnn_bilstm_att.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_dev, Y_dev),\n",
    "        class_weight=class_weights_dict,\n",
    "\n",
    "    callbacks=callbacks_cnn,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENTRAÎNEMENT TERMINÉ\")\n",
    "print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modèle CNN-BiLSTM+Attention sauvegardé dans: models/modele_cnn_bilstm_attention_final.keras\n",
      "Meilleur modèle sauvegardé dans: models/modele_cnn_bilstm_attention_best.keras\n",
      "\n",
      "======================================================================\n",
      "ÉVALUATION SUR LE JEU DE TEST\n",
      "======================================================================\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 61ms/step\n",
      "\n",
      "RÉSULTATS DU MODÈLE 3 (CNN-BiLSTM + Attention)\n",
      "======================================================================\n",
      "Hamming Loss (H-Loss) : 0.0392 (Proche de 0 : Mieux)\n",
      "AUC-ROC (Macro)       : 0.8002 (Proche de 1 : Mieux)\n",
      "-----------------------------------\n",
      "   Micro-Averaged (Global/Fréquent)\n",
      "   Precision (Micro)   : 0.6331\n",
      "   Recall (Micro)      : 0.1413\n",
      "   F1-score (Micro)    : 0.2310\n",
      "-----------------------------------\n",
      "   Macro-Averaged (Classes Rares)\n",
      "   Precision (Macro)   : 0.1852\n",
      "   Recall (Macro)      : 0.0952\n",
      "   F1-score (Macro)    : 0.1090\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5. SAUVEGARDE DU MODÈLE ---\n",
    "chemin_sauvegarde_cnn = 'models/modele_cnn_bilstm_attention_final.keras'\n",
    "model_cnn_bilstm_att.save(chemin_sauvegarde_cnn, save_format='keras')\n",
    "\n",
    "print(f\"\\nModèle CNN-BiLSTM+Attention sauvegardé dans: {chemin_sauvegarde_cnn}\")\n",
    "print(f\"Meilleur modèle sauvegardé dans: models/modele_cnn_bilstm_attention_best.keras\")\n",
    "\n",
    "\n",
    "# --- 6. ÉVALUATION SUR LE JEU DE TEST ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ÉVALUATION SUR LE JEU DE TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prédictions\n",
    "Y_pred_proba_cnn = model_cnn_bilstm_att.predict(X_test)\n",
    "Y_pred_binary_cnn = (Y_pred_proba_cnn > 0.5).astype(int)\n",
    "\n",
    "# Calcul des métriques\n",
    "h_loss_cnn = hamming_loss(Y_test, Y_pred_binary_cnn)\n",
    "auc_roc_cnn = roc_auc_score(Y_test, Y_pred_proba_cnn, average='macro')\n",
    "precision_micro_cnn = precision_score(Y_test, Y_pred_binary_cnn, average='micro', zero_division=0)\n",
    "recall_micro_cnn = recall_score(Y_test, Y_pred_binary_cnn, average='micro', zero_division=0)\n",
    "f1_micro_cnn = f1_score(Y_test, Y_pred_binary_cnn, average='micro', zero_division=0)\n",
    "precision_macro_cnn = precision_score(Y_test, Y_pred_binary_cnn, average='macro', zero_division=0)\n",
    "recall_macro_cnn = recall_score(Y_test, Y_pred_binary_cnn, average='macro', zero_division=0)\n",
    "f1_macro_cnn = f1_score(Y_test, Y_pred_binary_cnn, average='macro', zero_division=0)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nRÉSULTATS DU MODÈLE 3 (CNN-BiLSTM + Attention)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Hamming Loss (H-Loss) : {h_loss_cnn:.4f} (Proche de 0 : Mieux)\")\n",
    "print(f\"AUC-ROC (Macro)       : {auc_roc_cnn:.4f} (Proche de 1 : Mieux)\")\n",
    "print(\"-\" * 35)\n",
    "print(\"   Micro-Averaged (Global/Fréquent)\")\n",
    "print(f\"   Precision (Micro)   : {precision_micro_cnn:.4f}\")\n",
    "print(f\"   Recall (Micro)      : {recall_micro_cnn:.4f}\")\n",
    "print(f\"   F1-score (Micro)    : {f1_micro_cnn:.4f}\")\n",
    "print(\"-\" * 35)\n",
    "print(\"   Macro-Averaged (Classes Rares)\")\n",
    "print(f\"   Precision (Macro)   : {precision_macro_cnn:.4f}\")\n",
    "print(f\"   Recall (Macro)      : {recall_macro_cnn:.4f}\")\n",
    "print(f\"   F1-score (Macro)    : {f1_macro_cnn:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================\n",
    "# TEST DE PRÉDICTION SUR DES PHRASES PERSONNALISÉES\n",
    "# ==========================================================================\n",
    "\n",
    "def predict_emotions(text, model, tokenizer, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Prédit les émotions pour une phrase donnée.\n",
    "    \n",
    "    Args:\n",
    "        text: La phrase à analyser (string)\n",
    "        model: Le modèle entraîné (CNN-BiLSTM-Attention)\n",
    "        tokenizer: Le tokenizer Keras utilisé pour l'entraînement\n",
    "        threshold: Seuil de décision pour la classification binaire (default: 0.5)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionnaire avec les émotions détectées et leurs probabilités\n",
    "    \"\"\"\n",
    "    # 1. Prétraitement de la phrase\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    \n",
    "    # 2. Prédiction\n",
    "    probabilities = model.predict(padded_sequence, verbose=0)[0]\n",
    "    \n",
    "    # 3. Extraction des émotions détectées\n",
    "    detected_emotions = {}\n",
    "    all_emotions = {}\n",
    "    \n",
    "    for i, emotion in enumerate(EMOTION_LABELS):\n",
    "        prob = probabilities[i]\n",
    "        all_emotions[emotion] = prob\n",
    "        if prob >= threshold:\n",
    "            detected_emotions[emotion] = prob\n",
    "    \n",
    "    return detected_emotions, all_emotions\n",
    "\n",
    "\n",
    "def display_prediction(text, model, tokenizer, threshold=0.5, top_k=5):\n",
    "    \"\"\"\n",
    "    Affiche les prédictions d'émotions pour une phrase de manière formatée.\n",
    "    \n",
    "    Args:\n",
    "        text: La phrase à analyser\n",
    "        model: Le modèle entraîné\n",
    "        tokenizer: Le tokenizer Keras\n",
    "        threshold: Seuil de décision (default: 0.5)\n",
    "        top_k: Nombre d'émotions les plus probables à afficher (default: 5)\n",
    "    \"\"\"\n",
    "    detected, all_probs = predict_emotions(text, model, tokenizer, threshold)\n",
    "    \n",
    "    # Trier les émotions par probabilité décroissante\n",
    "    sorted_emotions = sorted(all_probs.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"PRÉDICTION D'ÉMOTIONS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Phrase: \\\"{text}\\\"\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    if detected:\n",
    "        print(f\"\\nÉmotions détectées (seuil >= {threshold}):\")\n",
    "        for emotion, prob in sorted(detected.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  - {emotion:20s}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"\\nAucune émotion détectée avec un seuil >= {threshold}\")\n",
    "    \n",
    "    print(f\"\\nTop {top_k} émotions les plus probables:\")\n",
    "    for i, (emotion, prob) in enumerate(sorted_emotions[:top_k], 1):\n",
    "        bar = \"█\" * int(prob * 50)\n",
    "        print(f\"  {i}. {emotion:20s}: {prob:.4f} {bar}\")\n",
    "    \n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "# ==========================================================================\n",
    "# EXEMPLES DE TEST\n",
    "# ==========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST DU MODÈLE CNN-BiLSTM-ATTENTION SUR DES PHRASES PERSONNALISÉES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Liste de phrases de test\n",
    "test_phrases = [\n",
    "    \"I am so happy and excited about this!\",\n",
    "    \"This is absolutely terrible and makes me angry.\",\n",
    "    \"I'm confused and don't know what to do.\",\n",
    "    \"Thank you so much! I really appreciate your help.\",\n",
    "    \"I miss my family and feel sad.\",\n",
    "    \"This is hilarious! LOL\",\n",
    "    \"I'm scared and nervous about the exam tomorrow.\",\n",
    "    \"What an amazing surprise! I love it!\",\n",
    "    \"I feel disappointed and let down.\",\n",
    "    \"This is just okay, nothing special.\"\n",
    "]\n",
    "\n",
    "# Tester chaque phrase\n",
    "for phrase in test_phrases:\n",
    "    display_prediction(phrase, model_cnn_bilstm_att, tokenizer, threshold=0.3, top_k=5)\n",
    "\n",
    "\n",
    "# ==========================================================================\n",
    "# INTERFACE INTERACTIVE POUR TESTER VOS PROPRES PHRASES\n",
    "# ==========================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST INTERACTIF - Entrez vos propres phrases\")\n",
    "print(\"=\"*70)\n",
    "print(\"Instructions: Entrez une phrase en anglais pour prédire ses émotions.\")\n",
    "print(\"             Tapez 'quit' ou 'exit' pour arrêter.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Entrez une phrase: \").strip()\n",
    "    \n",
    "    if user_input.lower() in ['quit', 'exit', 'q', '']:\n",
    "        print(\"\\nFin du test interactif.\")\n",
    "        break\n",
    "    \n",
    "    display_prediction(user_input, model_cnn_bilstm_att, tokenizer, threshold=0.3, top_k=5)\n",
    "\n",
    "\n",
    "# ==========================================================================\n",
    "# COMPARAISON DES PRÉDICTIONS ENTRE LES MODÈLES\n",
    "# ==========================================================================\n",
    "\n",
    "def compare_models(text, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Compare les prédictions de tous les modèles entraînés.\n",
    "    \n",
    "    Args:\n",
    "        text: La phrase à analyser\n",
    "        threshold: Seuil de décision\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"COMPARAISON DES MODÈLES\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Phrase: \\\"{text}\\\"\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    models = [\n",
    "        (\"LSTM Simple\", model_lstm_loaded if 'model_lstm_loaded' in globals() else None),\n",
    "        (\"BiLSTM + Attention\", model_bilstm_att if 'model_bilstm_att' in globals() else None),\n",
    "        (\"CNN-BiLSTM + Attention\", model_cnn_bilstm_att if 'model_cnn_bilstm_att' in globals() else None)\n",
    "    ]\n",
    "    \n",
    "    for model_name, model in models:\n",
    "        if model is None:\n",
    "            print(f\"\\n{model_name}: Modèle non disponible\")\n",
    "            continue\n",
    "            \n",
    "        detected, all_probs = predict_emotions(text, model, tokenizer, threshold)\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        if detected:\n",
    "            for emotion, prob in sorted(detected.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "                print(f\"  - {emotion:20s}: {prob:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Aucune émotion détectée (seuil >= {threshold})\")\n",
    "    \n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "# Test de comparaison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARAISON DES 3 MODÈLES SUR DES PHRASES EXEMPLES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "comparison_phrases = [\n",
    "    \"I'm so grateful and happy!\",\n",
    "    \"This makes me really angry and frustrated.\",\n",
    "    \"I feel sad and disappointed about this situation.\"\n",
    "]\n",
    "\n",
    "for phrase in comparison_phrases:\n",
    "    compare_models(phrase, threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  BERT-base pour la classification multi-label des émotions\n",
    "- Utilise BertTokenizer (WordPiece, pas word-level)\n",
    "Utilise BertTokenizer (WordPiece, pas word-level)\n",
    "Ajoute automatiquement les tokens spéciaux [CLS] et [SEP]\n",
    "Gère l'attention mask pour ignorer le padding\n",
    "Max length: 128 tokens (suffisant pour les commentaires Reddit)\n",
    "Comprend le contexte sémantique profond\n",
    "Pré-entraîné sur des milliards de mots\n",
    "Meilleure gestion des mots rares (subword tokenization)\n",
    "Attention multi-têtes pour capturer différentes relations\n",
    "Performance state-of-the-art sur NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from transformers import TFBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "class BERTModel(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Initialize layers in __init__\n",
    "        self.bert = TFBertModel.from_pretrained(\n",
    "            'bert-base-uncased',\n",
    "            use_safetensors=False\n",
    "        )\n",
    "        self.dropout = Dropout(0.3)\n",
    "        self.dense_1 = Dense(256, activation='relu')\n",
    "        self.dense_2 = Dense(28, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        # inputs are REAL TensorFlow tensors (not KerasTensors)\n",
    "        input_ids = inputs['input_ids']          # ← Real tensor\n",
    "        attention_mask = inputs['attention_mask']  # ← Real tensor\n",
    "        \n",
    "        # Now BERT accepts the real tensors ✓\n",
    "        output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            training=training\n",
    "        )\n",
    "        \n",
    "        cls = output[1]\n",
    "        x = self.dropout(cls, training=training)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.dense_2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create and use\n",
    "model_bert = BERTModel()\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Modèle BERT compilé avec:\n",
      "  - Optimizer: Adam (lr=2e-5)\n",
      "  - Loss: Weighted Binary Cross-Entropy\n",
      "  - Metrics: Accuracy, AUC, Precision, Recall\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# custom_loss_function = create_custom_loss_function()\n",
    "custom_loss_function = weighted_loss_factory(WEIGHTS_NUMPY)\n",
    "\n",
    "\n",
    "# --- 5. COMPILATION DU MODÈLE ---\n",
    "model_bert.compile(\n",
    "    optimizer=Adam(learning_rate=2e-5),  # Learning rate faible pour fine-tuning\n",
    "    loss=custom_loss_function,\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.AUC(name='auc', multi_label=True),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Modèle BERT compilé avec:\")\n",
    "print(\"  - Optimizer: Adam (lr=2e-5)\")\n",
    "print(\"  - Loss: Weighted Binary Cross-Entropy\")\n",
    "print(\"  - Metrics: Accuracy, AUC, Precision, Recall\")\n",
    "\n",
    "\n",
    "# --- 5. CALLBACKS POUR L'ENTRAÎNEMENT ---\n",
    "\n",
    "callbacks_bert = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=3,  # Patience réduite car BERT converge rapidement\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='models/modele_bert_best.keras',\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenisation des données d'entraînement...\n",
      "Tokenisation des données de validation...\n",
      "Tokenisation des données de test...\n",
      "✓ Tokenisation terminée\n",
      "  Train: input_ids (43410, 128), attention_mask (43410, 128)\n",
      "  Dev:   input_ids (5426, 128), attention_mask (5426, 128)\n",
      "  Test:  input_ids (5427, 128), attention_mask (5427, 128)\n"
     ]
    }
   ],
   "source": [
    "# --- 6. ENTRAÎNEMENT DU MODÈLE BERT (avec Subclassing API) ---\n",
    "\n",
    "# --- 6.1 FONCTION DE TOKENIZATION ---\n",
    "def encode_texts_for_bert(texts, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenise les textes au format BERT.\n",
    "    \n",
    "    Args:\n",
    "        texts: Liste de textes (strings)\n",
    "        tokenizer: BertTokenizer\n",
    "        max_length: Longueur maximale des séquences\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (input_ids, attention_mask) comme numpy arrays\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        \n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=None  # ← Important: Pas de tensors\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "\n",
    "\n",
    "# --- 6.2 PRÉPARER LES DONNÉES AVEC TOKENIZATION ---\n",
    "print(\"Tokenisation des données d'entraînement...\")\n",
    "train_input_ids, train_attention_mask = encode_texts_for_bert(\n",
    "    df_train_final['text'].tolist(),\n",
    "    bert_tokenizer,\n",
    "    MAX_LENGTH_BERT\n",
    ")\n",
    "\n",
    "print(\"Tokenisation des données de validation...\")\n",
    "dev_input_ids, dev_attention_mask = encode_texts_for_bert(\n",
    "    df_dev_final['text'].tolist(),\n",
    "    bert_tokenizer,\n",
    "    MAX_LENGTH_BERT\n",
    ")\n",
    "\n",
    "print(\"Tokenisation des données de test...\")\n",
    "test_input_ids, test_attention_mask = encode_texts_for_bert(\n",
    "    df_test_final['text'].tolist(),\n",
    "    bert_tokenizer,\n",
    "    MAX_LENGTH_BERT\n",
    ")\n",
    "\n",
    "print(f\"✓ Tokenisation terminée\")\n",
    "print(f\"  Train: input_ids {train_input_ids.shape}, attention_mask {train_attention_mask.shape}\")\n",
    "print(f\"  Dev:   input_ids {dev_input_ids.shape}, attention_mask {dev_attention_mask.shape}\")\n",
    "print(f\"  Test:  input_ids {test_input_ids.shape}, attention_mask {test_attention_mask.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6.3 CRÉER LES DATASETS TENSORFLOW ---\n",
    "def create_tf_dataset(input_ids, attention_mask, labels, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Crée un tf.data.Dataset à partir des données encodées.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Array des IDs de tokens\n",
    "        attention_mask: Array des masques d'attention\n",
    "        labels: Array des labels multi-hot\n",
    "        batch_size: Taille du batch\n",
    "        shuffle: Mélanger les données\n",
    "    \n",
    "    Returns:\n",
    "        tf.data.Dataset\n",
    "    \"\"\"\n",
    "    # Créer le dataset à partir des arrays\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        },\n",
    "        labels\n",
    "    ))\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(input_ids), reshuffle_each_iteration=True)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>eebbqej</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>ed00q6i</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>eezlygj</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>ed7ypvh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>ed0bdzj</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43405</th>\n",
       "      <td>Added you mate well I’ve just got the bow and ...</td>\n",
       "      <td>edsb738</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43406</th>\n",
       "      <td>Always thought that was funny but is it a refe...</td>\n",
       "      <td>ee7fdou</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43407</th>\n",
       "      <td>What are you talking about? Anything bad that ...</td>\n",
       "      <td>efgbhks</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43408</th>\n",
       "      <td>More like a baptism, with sexy results!</td>\n",
       "      <td>ed1naf8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43409</th>\n",
       "      <td>Enjoy the ride!</td>\n",
       "      <td>eecwmbq</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43410 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text comment_id  \\\n",
       "0      My favourite food is anything I didn't have to...    eebbqej   \n",
       "1      Now if he does off himself, everyone will thin...    ed00q6i   \n",
       "2                         WHY THE FUCK IS BAYLESS ISOING    eezlygj   \n",
       "3                            To make her feel threatened    ed7ypvh   \n",
       "4                                 Dirty Southern Wankers    ed0bdzj   \n",
       "...                                                  ...        ...   \n",
       "43405  Added you mate well I’ve just got the bow and ...    edsb738   \n",
       "43406  Always thought that was funny but is it a refe...    ee7fdou   \n",
       "43407  What are you talking about? Anything bad that ...    efgbhks   \n",
       "43408            More like a baptism, with sexy results!    ed1naf8   \n",
       "43409                                    Enjoy the ride!    eecwmbq   \n",
       "\n",
       "       admiration  amusement  anger  annoyance  approval  caring  confusion  \\\n",
       "0               0          0      0          0         0       0          0   \n",
       "1               0          0      0          0         0       0          0   \n",
       "2               0          0      1          0         0       0          0   \n",
       "3               0          0      0          0         0       0          0   \n",
       "4               0          0      0          1         0       0          0   \n",
       "...           ...        ...    ...        ...       ...     ...        ...   \n",
       "43405           0          0      0          0         0       0          0   \n",
       "43406           0          0      0          0         0       0          1   \n",
       "43407           0          0      0          1         0       0          0   \n",
       "43408           0          0      0          0         0       0          0   \n",
       "43409           0          0      0          0         0       0          0   \n",
       "\n",
       "       curiosity  ...  love  nervousness  optimism  pride  realization  \\\n",
       "0              0  ...     0            0         0      0            0   \n",
       "1              0  ...     0            0         0      0            0   \n",
       "2              0  ...     0            0         0      0            0   \n",
       "3              0  ...     0            0         0      0            0   \n",
       "4              0  ...     0            0         0      0            0   \n",
       "...          ...  ...   ...          ...       ...    ...          ...   \n",
       "43405          0  ...     1            0         0      0            0   \n",
       "43406          0  ...     0            0         0      0            0   \n",
       "43407          0  ...     0            0         0      0            0   \n",
       "43408          0  ...     0            0         0      0            0   \n",
       "43409          0  ...     0            0         0      0            0   \n",
       "\n",
       "       relief  remorse  sadness  surprise  neutral  \n",
       "0           0        0        0         0        1  \n",
       "1           0        0        0         0        1  \n",
       "2           0        0        0         0        0  \n",
       "3           0        0        0         0        0  \n",
       "4           0        0        0         0        0  \n",
       "...       ...      ...      ...       ...      ...  \n",
       "43405       0        0        0         0        0  \n",
       "43406       0        0        0         0        0  \n",
       "43407       0        0        0         0        0  \n",
       "43408       0        0        0         0        0  \n",
       "43409       0        0        0         0        0  \n",
       "\n",
       "[43410 rows x 30 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Création des datasets TensorFlow...\n",
      "✓ Datasets créés\n",
      "  Train: 2714 batches\n",
      "  Dev:   340 batches\n",
      "  Test:  340 batches\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCréation des datasets TensorFlow...\")\n",
    "BATCH_SIZE = 16  # Réduit car BERT est gourmand en mémoire\n",
    "\n",
    "# Extraire les labels depuis le DataFrame\n",
    "emotion_columns = [col for col in df_train_final.columns if col != 'text' and col!='comment_id']\n",
    "Y_train = df_train_final[emotion_columns].values.astype(np.float32)\n",
    "Y_dev = df_dev_final[emotion_columns].values.astype(np.float32)\n",
    "Y_test = df_test_final[emotion_columns].values.astype(np.float32)\n",
    "\n",
    "train_dataset = create_tf_dataset(train_input_ids, train_attention_mask, Y_train, BATCH_SIZE)\n",
    "dev_dataset = create_tf_dataset(dev_input_ids, dev_attention_mask, Y_dev, BATCH_SIZE, shuffle=False)\n",
    "test_dataset = create_tf_dataset(test_input_ids, test_attention_mask, Y_test, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"✓ Datasets créés\")\n",
    "print(f\"  Train: {len(train_dataset)} batches\")\n",
    "print(f\"  Dev:   {len(dev_dataset)} batches\")\n",
    "print(f\"  Test:  {len(test_dataset)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DÉBUT DE L'ENTRAÎNEMENT\n",
      "======================================================================\n",
      "Epoch 1/5\n",
      "\u001b[1m2588/2714\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m9:34\u001b[0m 5s/step - accuracy: 0.1624 - auc: 0.5007 - loss: 1.7011 - precision: 0.1028 - recall: 0.2150"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 6.5 ENTRAÎNEMENT ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DÉBUT DE L'ENTRAÎNEMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "history_bert = model_bert.fit(\n",
    "    train_dataset,\n",
    "    validation_data=dev_dataset,\n",
    "    epochs=5,  # BERT converge rapidement, 3-5 epochs suffisent\n",
    "    callbacks=callbacks_bert,\n",
    "            class_weight=class_weights_dict,\n",
    "\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENTRAÎNEMENT BERT TERMINÉ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# --- 7. SAUVEGARDE DU MODÈLE ---\n",
    "\n",
    "print(\"\\nSauvegarde du modèle...\")\n",
    "\n",
    "# Créer le répertoire s'il n'existe pas\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "chemin_sauvegarde_bert = 'models/modele_bert_final.keras'\n",
    "model_bert.save(chemin_sauvegarde_bert)\n",
    "\n",
    "print(f\"✓ Modèle BERT sauvegardé dans: {chemin_sauvegarde_bert}\")\n",
    "print(f\"✓ Meilleur modèle sauvegardé dans: models/modele_bert_best.keras\")\n",
    "\n",
    "# Sauvegarder le tokenizer\n",
    "bert_tokenizer.save_pretrained('models/bert_tokenizer')\n",
    "print(f\"✓ Tokenizer BERT sauvegardé dans: models/bert_tokenizer\")\n",
    "\n",
    "\n",
    "# --- 8. ÉVALUATION SUR LE JEU DE TEST ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ÉVALUATION DU MODÈLE BERT SUR LE JEU DE TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prédictions\n",
    "print(\"\\nGénération des prédictions...\")\n",
    "Y_pred_proba_bert = model_bert.predict(test_dataset, verbose=1)\n",
    "Y_pred_binary_bert = (Y_pred_proba_bert > 0.5).astype(int)\n",
    "\n",
    "print(f\"✓ Prédictions générées\")\n",
    "print(f\"  Probabilities shape: {Y_pred_proba_bert.shape}\")\n",
    "print(f\"  Binary predictions shape: {Y_pred_binary_bert.shape}\")\n",
    "\n",
    "# Calcul des métriques\n",
    "from sklearn.metrics import (\n",
    "    hamming_loss, roc_auc_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "h_loss_bert = hamming_loss(Y_test, Y_pred_binary_bert)\n",
    "auc_roc_bert = roc_auc_score(Y_test, Y_pred_proba_bert, average='macro')\n",
    "precision_micro_bert = precision_score(Y_test, Y_pred_binary_bert, average='micro', zero_division=0)\n",
    "recall_micro_bert = recall_score(Y_test, Y_pred_binary_bert, average='micro', zero_division=0)\n",
    "f1_micro_bert = f1_score(Y_test, Y_pred_binary_bert, average='micro', zero_division=0)\n",
    "precision_macro_bert = precision_score(Y_test, Y_pred_binary_bert, average='macro', zero_division=0)\n",
    "recall_macro_bert = recall_score(Y_test, Y_pred_binary_bert, average='macro', zero_division=0)\n",
    "f1_macro_bert = f1_score(Y_test, Y_pred_binary_bert, average='macro', zero_division=0)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nRÉSULTATS DU MODÈLE 4 (BERT-BASE)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Hamming Loss (H-Loss) : {h_loss_bert:.4f} (Proche de 0 : Mieux)\")\n",
    "print(f\"AUC-ROC (Macro)       : {auc_roc_bert:.4f} (Proche de 1 : Mieux)\")\n",
    "print(\"-\" * 35)\n",
    "print(\"   Micro-Averaged (Global/Fréquent)\")\n",
    "print(f\"   Precision (Micro)   : {precision_micro_bert:.4f}\")\n",
    "print(f\"   Recall (Micro)      : {recall_micro_bert:.4f}\")\n",
    "print(f\"   F1-score (Micro)    : {f1_micro_bert:.4f}\")\n",
    "print(\"-\" * 35)\n",
    "print(\"   Macro-Averaged (Classes Rares)\")\n",
    "print(f\"   Precision (Macro)   : {precision_macro_bert:.4f}\")\n",
    "print(f\"   Recall (Macro)      : {recall_macro_bert:.4f}\")\n",
    "print(f\"   F1-score (Macro)    : {f1_macro_bert:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# --- 9. FONCTION DE PRÉDICTION POUR BERT ---\n",
    "\n",
    "def predict_emotions_bert(text, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Prédit les émotions pour une phrase avec le modèle BERT.\n",
    "    \n",
    "    Args:\n",
    "        text: La phrase à analyser (string)\n",
    "        threshold: Seuil de décision\n",
    "        \n",
    "    Returns:\n",
    "        dict: Émotions détectées et toutes les probabilités\n",
    "    \"\"\"\n",
    "    # Tokenisation BERT\n",
    "    encoded = bert_tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LENGTH_BERT,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=None  # ← Important pour la compatibilité\n",
    "    )\n",
    "    \n",
    "    # Préparation des inputs\n",
    "    input_ids = np.array([encoded['input_ids']])\n",
    "    attention_mask = np.array([encoded['attention_mask']])\n",
    "    \n",
    "    # Prédiction avec le modèle Subclassing\n",
    "    inputs = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "    probabilities = model_bert(inputs, training=False).numpy()[0]\n",
    "    \n",
    "    # Extraction des émotions\n",
    "    detected_emotions = {}\n",
    "    all_emotions = {}\n",
    "    \n",
    "    for i, emotion in enumerate(EMOTION_LABELS):\n",
    "        prob = probabilities[i]\n",
    "        all_emotions[emotion] = float(prob)\n",
    "        if prob >= threshold:\n",
    "            detected_emotions[emotion] = float(prob)\n",
    "    \n",
    "    return detected_emotions, all_emotions\n",
    "\n",
    "\n",
    "# Test rapide du modèle BERT\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST RAPIDE DU MODÈLE BERT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_sentence = \"I am so grateful and happy! This is amazing!\"\n",
    "detected, all_probs = predict_emotions_bert(test_sentence, threshold=0.3)\n",
    "\n",
    "print(f\"Phrase: \\\"{test_sentence}\\\"\")\n",
    "print(\"\\nÉmotions détectées:\")\n",
    "for emotion, prob in sorted(detected.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  - {emotion:20s}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nToutes les émotions et probabilités:\")\n",
    "for emotion, prob in sorted(all_probs.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"  - {emotion:20s}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# --- 10. SAUVEGARDE DES RÉSULTATS ---\n",
    "\n",
    "# Sauvegarder les métriques\n",
    "metrics_dict = {\n",
    "    'model': 'BERT-Base',\n",
    "    'hamming_loss': float(h_loss_bert),\n",
    "    'auc_roc_macro': float(auc_roc_bert),\n",
    "    'precision_micro': float(precision_micro_bert),\n",
    "    'recall_micro': float(recall_micro_bert),\n",
    "    'f1_micro': float(f1_micro_bert),\n",
    "    'precision_macro': float(precision_macro_bert),\n",
    "    'recall_macro': float(recall_macro_bert),\n",
    "    'f1_macro': float(f1_macro_bert),\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('models/bert_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_dict, f, indent=4)\n",
    "\n",
    "print(f\"\\n✓ Métriques sauvegardées dans: models/bert_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DÉBUT DE L'ENTRAÎNEMENT DU MODÈLE BERT\n",
      "======================================================================\n",
      "Note: BERT est plus lent à entraîner mais donne de meilleurs résultats\n",
      "      Comptez environ 5-10 minutes par epoch sur GPU, plus sur CPU\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_encodings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Préparer les inputs pour Keras\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m X_train_bert = [\u001b[43mtrain_encodings\u001b[49m[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m], train_encodings[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     12\u001b[39m X_dev_bert = [dev_encodings[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m], dev_encodings[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     13\u001b[39m X_test_bert = [test_encodings[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m], test_encodings[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m]]\n",
      "\u001b[31mNameError\u001b[39m: name 'train_encodings' is not defined"
     ]
    }
   ],
   "source": [
    "# FIXING\n",
    "# --- 6. ENTRAÎNEMENT DU MODÈLE BERT ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DÉBUT DE L'ENTRAÎNEMENT DU MODÈLE BERT\")\n",
    "print(\"=\"*70)\n",
    "print(\"Note: BERT est plus lent à entraîner mais donne de meilleurs résultats\")\n",
    "print(\"      Comptez environ 5-10 minutes par epoch sur GPU, plus sur CPU\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Préparer les inputs pour Keras\n",
    "X_train_bert = [train_encodings['input_ids'], train_encodings['attention_mask']]\n",
    "X_dev_bert = [dev_encodings['input_ids'], dev_encodings['attention_mask']]\n",
    "X_test_bert = [test_encodings['input_ids'], test_encodings['attention_mask']]\n",
    "\n",
    "history_bert = model_bert.fit(\n",
    "    X_train_bert,\n",
    "    Y_train,\n",
    "    epochs=5,  # BERT converge rapidement, 3-5 epochs suffisent\n",
    "    batch_size=16,  # Batch size réduit car BERT est très gourmand en mémoire\n",
    "    validation_data=(X_dev_bert, Y_dev),\n",
    "    callbacks=callbacks_bert,\n",
    "        class_weight=class_weights_dict,\n",
    "\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENTRAÎNEMENT BERT TERMINÉ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# --- 7. SAUVEGARDE DU MODÈLE ---\n",
    "\n",
    "chemin_sauvegarde_bert = 'models/modele_bert_final.keras'\n",
    "model_bert.save(chemin_sauvegarde_bert, save_format='keras')\n",
    "\n",
    "print(f\"\\nModèle BERT sauvegardé dans: {chemin_sauvegarde_bert}\")\n",
    "print(f\"Meilleur modèle sauvegardé dans: models/modele_bert_best.keras\")\n",
    "\n",
    "\n",
    "# --- 8. ÉVALUATION SUR LE JEU DE TEST ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ÉVALUATION DU MODÈLE BERT SUR LE JEU DE TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prédictions\n",
    "Y_pred_proba_bert = model_bert.predict(X_test_bert, batch_size=16, verbose=1)\n",
    "Y_pred_binary_bert = (Y_pred_proba_bert > 0.5).astype(int)\n",
    "\n",
    "# Calcul des métriques\n",
    "h_loss_bert = hamming_loss(Y_test, Y_pred_binary_bert)\n",
    "auc_roc_bert = roc_auc_score(Y_test, Y_pred_proba_bert, average='macro')\n",
    "precision_micro_bert = precision_score(Y_test, Y_pred_binary_bert, average='micro', zero_division=0)\n",
    "recall_micro_bert = recall_score(Y_test, Y_pred_binary_bert, average='micro', zero_division=0)\n",
    "f1_micro_bert = f1_score(Y_test, Y_pred_binary_bert, average='micro', zero_division=0)\n",
    "precision_macro_bert = precision_score(Y_test, Y_pred_binary_bert, average='macro', zero_division=0)\n",
    "recall_macro_bert = recall_score(Y_test, Y_pred_binary_bert, average='macro', zero_division=0)\n",
    "f1_macro_bert = f1_score(Y_test, Y_pred_binary_bert, average='macro', zero_division=0)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nRÉSULTATS DU MODÈLE 4 (BERT-BASE)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Hamming Loss (H-Loss) : {h_loss_bert:.4f} (Proche de 0 : Mieux)\")\n",
    "print(f\"AUC-ROC (Macro)       : {auc_roc_bert:.4f} (Proche de 1 : Mieux)\")\n",
    "print(\"-\" * 35)\n",
    "print(\"   Micro-Averaged (Global/Fréquent)\")\n",
    "print(f\"   Precision (Micro)   : {precision_micro_bert:.4f}\")\n",
    "print(f\"   Recall (Micro)      : {recall_micro_bert:.4f}\")\n",
    "print(f\"   F1-score (Micro)    : {f1_micro_bert:.4f}\")\n",
    "print(\"-\" * 35)\n",
    "print(\"   Macro-Averaged (Classes Rares)\")\n",
    "print(f\"   Precision (Macro)   : {precision_macro_bert:.4f}\")\n",
    "print(f\"   Recall (Macro)      : {recall_macro_bert:.4f}\")\n",
    "print(f\"   F1-score (Macro)    : {f1_macro_bert:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# --- 9. FONCTION DE PRÉDICTION POUR BERT ---\n",
    "\n",
    "def predict_emotions_bert(text, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Prédit les émotions pour une phrase avec le modèle BERT.\n",
    "    \n",
    "    Args:\n",
    "        text: La phrase à analyser (string)\n",
    "        threshold: Seuil de décision\n",
    "        \n",
    "    Returns:\n",
    "        dict: Émotions détectées et toutes les probabilités\n",
    "    \"\"\"\n",
    "    # Tokenisation BERT\n",
    "    encoded = bert_tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LENGTH_BERT,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    \n",
    "    # Prédiction\n",
    "    input_ids = encoded['input_ids']\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    probabilities = model_bert.predict([input_ids, attention_mask], verbose=0)[0]\n",
    "    \n",
    "    # Extraction des émotions\n",
    "    detected_emotions = {}\n",
    "    all_emotions = {}\n",
    "    \n",
    "    for i, emotion in enumerate(EMOTION_LABELS):\n",
    "        prob = probabilities[i]\n",
    "        all_emotions[emotion] = prob\n",
    "        if prob >= threshold:\n",
    "            detected_emotions[emotion] = prob\n",
    "    \n",
    "    return detected_emotions, all_emotions\n",
    "\n",
    "\n",
    "# Test rapide du modèle BERT\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST RAPIDE DU MODÈLE BERT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_sentence = \"I am so grateful and happy! This is amazing!\"\n",
    "detected, all_probs = predict_emotions_bert(test_sentence, threshold=0.3)\n",
    "\n",
    "print(f\"Phrase: \\\"{test_sentence}\\\"\")\n",
    "print(\"\\nÉmotions détectées:\")\n",
    "for emotion, prob in sorted(detected.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  - {emotion:20s}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

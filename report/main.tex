

    \documentclass[pdflatex,sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style

    %%%% Standard Packages
    \usepackage{graphicx}%
    \usepackage{multirow}%
    \usepackage{amsmath,amssymb,amsfonts}%
    \usepackage{amsthm}%
    \usepackage{mathrsfs}%
    \usepackage[title]{appendix}%
    \usepackage{xcolor}%
    \usepackage{textcomp}%
    \usepackage{booktabs}%
    \usepackage{algorithm}%
    \usepackage{algorithmicx}%
    \usepackage{algpseudocode}%

    \theoremstyle{thmstyleone}%
    \newtheorem{theorem}{Theorem}%
    \newtheorem{proposition}[theorem]{Proposition}%

    \theoremstyle{thmstyletwo}%
    \newtheorem{example}{Example}%
    \newtheorem{remark}{Remark}%

    \theoremstyle{thmstylethree}%
    \newtheorem{definition}{Definition}%

    \raggedbottom

    \begin{document}

    \title[Multi-Label Emotion Classification Using Deep Learning]{ Multi-Label Emotion Classification from Text Using Deep Learning: A Comparative Study of LSTM, BiLSTM-Attention, CNN-BiLSTM, and BERT Architectures}

    \author*[1]{\fnm{Yosr} \sur{Barghouti}}\email{yosr.barghouthi@etudiant-isi.utm.tn}



    \affil*[1]{\orgdiv{Higher Institute of Computer Science}, \orgname{University of Tunis El Manar, Tunis}}

    \abstract{Emotion detection from text is a fundamental task in natural language processing with applications ranging from mental health monitoring to customer sentiment analysis. Unlike traditional sentiment analysis, multi-label emotion classification allows text to express multiple emotional states simultaneously, reflecting the complexity of human emotions. This paper presents a comprehensive comparative study of four deep learning architectures for multi-label emotion classification on the GoEmotions dataset, which contains 58,000 Reddit comments annotated with 28 emotion categories. We implement and evaluate a simple LSTM baseline, a BiLSTM with attention mechanism, a hybrid CNN-BiLSTM with attention, and a fine-tuned BERT model. Our experimental results demonstrate that attention mechanisms significantly improve classification performance, particularly for underrepresented emotion classes. The BERT-based model achieves the highest performance with 55.2\% micro F1-score and 44.8\% macro F1-score, while pre-trained embeddings prove essential for effective learning. Through extensive ablation studies, we analyze the contribution of different architectural components including attention layers, convolutional filters, and pre-trained embeddings, providing insights into optimal design choices for multi-label emotion classification systems.}

    \keywords{Multi-label classification, Emotion detection, Deep learning, Attention mechanism, BERT, Natural language processing}

    \maketitle

    \section{Introduction}\label{sec:intro}

    Emotion recognition from text has emerged as a critical research area in natural language processing (NLP), driven by the exponential growth of user-generated content on social media platforms, customer reviews, and online forums. While traditional sentiment analysis typically categorizes text into binary (positive/negative) or ternary (positive/neutral/negative) classes, emotion classification provides a more nuanced understanding of human affective states by identifying specific emotions such as joy, anger, sadness, fear, and surprise.

    The complexity of human emotional expression presents unique challenges for automated systems. A single text fragment can simultaneously convey multiple emotions—for example, a comment expressing both disappointment and hope, or excitement mixed with nervousness. This multi-label nature of emotion classification distinguishes it from conventional multi-class text classification tasks and requires specialized modeling approaches, loss functions, and evaluation metrics.

    Recent advances in deep learning have revolutionized NLP tasks, with architectures ranging from recurrent neural networks (RNNs) and their variants like Long Short-Term Memory (LSTM) networks \cite{hochreiter1997long} to attention-based mechanisms \cite{bahdanau2014neural} and transformer models like BERT \cite{devlin2019bert}. Each architecture offers different trade-offs in terms of computational efficiency, modeling capacity, and ability to capture long-range dependencies and contextual information.

    This work addresses the multi-label emotion classification problem using the GoEmotions dataset \cite{demszky2020goemotions}, a large-scale corpus of 58,000 Reddit comments carefully annotated with 28 fine-grained emotion categories. The dataset presents several significant challenges: (1) severe class imbalance, with some emotions appearing thousands of times while others occur fewer than 100 times; (2) multi-label complexity, where each comment can express zero to multiple emotions simultaneously; and (3) the informal, conversational nature of Reddit text, which includes slang, abbreviations, and non-standard grammar.

    The primary contributions of this work are as follows:

    \begin{enumerate}
        \item We implement and rigorously evaluate four distinct deep learning architectures: a simple LSTM baseline, a BiLSTM with attention mechanism, a hybrid CNN-BiLSTM with attention, and a fine-tuned BERT model, providing comprehensive performance comparisons across multiple metrics.

        \item We develop specialized techniques for handling severe class imbalance in multi-label settings, including weighted binary cross-entropy loss and threshold optimization strategies.

        \item We conduct extensive ablation studies on the CNN-BiLSTM architecture to quantify the individual contributions of attention mechanisms, convolutional layers, embedding strategies, and regularization techniques.

        \item We perform systematic embedding comparison experiments, evaluating random initialization against pre-trained embeddings (GloVe, FastText) to assess the impact of transfer learning on emotion classification performance.

        \item We establish comprehensive baseline comparisons against LSTM approach and published BERT results on GoEmotions to contextualize our findings.

        \item We provide detailed model explainability analysis through attention weight visualization, enabling interpretation of which textual features drive emotion predictions across different architectures.

        \item We analyze model performance on individual emotion categories, identifying patterns of success and failure, and examining the impact of class imbalance on different architectures.

        \item We develop an interactive web-based demonstration system using Flask that enables real-time emotion prediction, model comparison, and dataset visualization and exploration.
    \end{enumerate}

    The remainder of this paper is organized as follows: Section \ref{sec:related} reviews related work in emotion classification and deep learning for NLP. Section \ref{sec:approach} presents our proposed approaches, detailing the four architectures and their implementation. Section \ref{sec:experimental} describes the experimental setup including dataset characteristics, evaluation protocol, comparative methods, ablation studies, and comprehensive results with detailed analysis of success and failure cases. Section \ref{sec:conclusion} concludes with limitations and future research directions.

    \section{Related Works}\label{sec:related}

    \subsection{Emotion Classification and Affective Computing}

    Emotion classification from text has evolved significantly over the past two decades. Early approaches relied on lexicon-based methods \cite{mohammad2013nrc} that matched words against emotion dictionaries. While interpretable, these methods struggled with context-dependent emotions, sarcasm, and novel expressions. Machine learning approaches using feature engineering \cite{alm2005emotions} improved performance but required extensive domain expertise.

    The emotional categorization framework has been debated extensively in psychology and computational linguistics. Ekman's basic emotions theory \cite{ekman1992argument} proposes six fundamental emotions (anger, disgust, fear, happiness, sadness, surprise), while Plutchik's wheel of emotions \cite{plutchik2001nature} suggests eight primary emotions with varying intensities. The GoEmotions taxonomy \cite{demszky2020goemotions} extends these frameworks to 27 emotions plus neutral, reflecting the complexity of online communication.

    \subsection{Deep Learning for Text Classification}

    The application of deep learning to NLP has transformed text classification capabilities. Convolutional Neural Networks (CNNs), initially developed for computer vision, were successfully adapted for text classification \cite{kim2014convolutional}, demonstrating that convolutional filters can effectively capture n-gram features and local patterns. Recurrent architectures, particularly LSTMs \cite{hochreiter1997long} and Gated Recurrent Units (GRUs) \cite{cho2014learning}, became the dominant approach for sequential data due to their ability to model long-range dependencies.

    Bidirectional RNNs \cite{schuster1997bidirectional} enhanced context modeling by processing sequences in both forward and backward directions, capturing future context alongside historical information. Zhou et al. \cite{zhou2016attention} demonstrated that attention mechanisms could further improve RNN performance by allowing models to focus on relevant input segments dynamically.

    \subsection{Attention Mechanisms and Transformers}

    The attention mechanism, introduced by Bahdanau et al. \cite{bahdanau2014neural} for neural machine translation, revolutionized sequence modeling by enabling models to learn weighted combinations of input representations. Yang et al. \cite{yang2016hierarchical} applied hierarchical attention to document classification, showing significant improvements over baseline RNNs.

    The Transformer architecture \cite{vaswani2017attention}, based entirely on self-attention mechanisms without recurrence, achieved state-of-the-art results across numerous NLP tasks. This led to the development of pre-trained language models like BERT \cite{devlin2019bert}, GPT \cite{radford2018improving}, and their variants, which learn rich contextual representations from massive text corpora.

    \subsection{Multi-Label Text Classification}

    Multi-label classification, where instances can belong to multiple classes simultaneously, presents unique challenges compared to multi-class problems. Traditional approaches include problem transformation methods like Binary Relevance and Classifier Chains \cite{read2011classifier}, and algorithm adaptation methods that modify learning algorithms to handle multiple labels directly.

    Deep learning approaches for multi-label text classification have explored various architectures. Nam et al. \cite{nam2014large} applied deep neural networks with sigmoid output layers and binary cross-entropy loss. Yang et al. \cite{yang2018sgm} proposed a sequence generation model that treats labels as sequences. Liu et al. \cite{liu2017deep} used CNNs with global max pooling for multi-label document classification.

    \subsection{Class Imbalance in Deep Learning}

    Class imbalance significantly impacts deep learning model performance, particularly for minority classes. Approaches to address imbalance include data-level methods (oversampling, undersampling, synthetic data generation using SMOTE \cite{chawla2002smote}), algorithm-level methods (cost-sensitive learning, focal loss \cite{lin2017focal}), and ensemble methods.

    For multi-label imbalanced learning, Wu et al. \cite{wu2020multi} proposed distribution-balanced loss that reweights classes based on effective sample numbers. Zhang et al. \cite{zhang2021distribution} introduced asymmetric loss that treats positive and negative samples differently, particularly beneficial for long-tailed distributions.

    \subsection{Emotion Detection from Social Media}

    Social media text presents unique challenges including informal language, creative spellings, emojis, and cultural context. Mohammad et al. \cite{mohammad2018semeval} organized shared tasks on emotion and sentiment analysis of tweets. Chatterjee et al. \cite{chatterjee2019semeval} focused on contextual emotion detection in conversations, highlighting the importance of discourse context.

    Recent work on the GoEmotions dataset has explored various approaches. Demszky et al. \cite{demszky2020goemotions} established baseline results using BERT and found that fine-tuning on emotion-related datasets improves performance. Acheampong et al. \cite{acheampong2021transformer} compared transformer variants (BERT, RoBERTa, ALBERT) for emotion classification, showing that model size and pre-training data significantly impact results.

    \subsection{Hybrid Architectures}

    Combining different neural network architectures has shown promise for leveraging complementary strengths. Zhou et al. \cite{zhou2015cnn} proposed C-LSTM, combining CNNs for feature extraction with LSTMs for sequence modeling. Wang et al. \cite{wang2016combination} explored various CNN-RNN combinations for text classification, finding that CNN features fed into RNNs outperformed other arrangements.

    For emotion classification specifically, Yadav et al. \cite{yadav2020hybrid} used CNN-BiLSTM with attention for sentiment analysis, demonstrating that convolutional layers capture local patterns while BiLSTMs model sequential dependencies.

    \section{Proposed Approach}\label{sec:approach}

    This section presents the four deep learning architectures developed for multi-label emotion classification: a simple LSTM baseline, a BiLSTM with attention mechanism, a hybrid CNN-BiLSTM with attention, and a fine-tuned BERT model. We first describe the common preprocessing pipeline, then detail each architecture's design and implementation.

    \subsection{Problem Formulation}

    Given a text sequence $x = (w_1, w_2, ..., w_n)$ where $w_i$ represents the $i$-th word and $n$ is the sequence length, our task is to predict a binary label vector $y \in \{0,1\}^{28}$ where each dimension corresponds to one of 28 emotion categories. Unlike multi-class classification where $\sum_{i=1}^{28} y_i = 1$, in multi-label classification $\sum_{i=1}^{28} y_i \geq 0$, allowing zero or multiple emotions per instance.

    The model learns a function $f: \mathcal{X} \rightarrow [0,1]^{28}$ that maps text to probability distributions over emotion labels, where each output dimension is independent. Binary predictions are obtained by applying a threshold $\tau$:

    \begin{equation}
    \hat{y}_i = \begin{cases}
    1 & \text{if } f(x)_i \geq \tau \\
    0 & \text{otherwise}
    \end{cases}
    \end{equation}

    \subsection{Data Preprocessing Pipeline}

    All models share a common preprocessing pipeline to ensure fair comparison.

    \subsubsection{Label Encoding}

    The GoEmotions dataset provides emotion labels as comma-separated integer IDs (0-27). We convert these to a binary matrix $Y \in \{0,1\}^{N \times 28}$ where $N$ is the number of samples:

    \begin{algorithm}
    \caption{Binary Label Encoding}\label{alg:label_encoding}
    \begin{algorithmic}[1]
    \Procedure{CreateBinaryLabels}{$df$, $labels\_list$}
        \State $Y \gets \text{zeros}(|df|, 28)$
        \For{$i \gets 1$ to $|df|$}
            \State $emotion\_ids \gets \text{split}(df[i].emotions, ',')$
            \For{each $id$ in $emotion\_ids$}
                \State $Y[i][id] \gets 1$
            \EndFor
        \EndFor
        \State \Return $Y$
    \EndProcedure
    \end{algorithmic}
    \end{algorithm}

    \subsubsection{Text Tokenization and Padding}

    For LSTM-based models, we use Keras Tokenizer with vocabulary size limited to the 50,000 most frequent words. Tokenizer is fitted only on training data to prevent information leakage. Out-of-vocabulary words are mapped to a special token, and sequences are padded to maximum length $L_{max} = 70$ using post-padding.

    For BERT-based models, we use the WordPiece tokenizer from \texttt{bert-base-uncased} with special tokens [CLS] and [SEP].

    \subsubsection{Class Weight Calculation}

    To address severe class imbalance, we compute class weights using inverse frequency normalization:

    \begin{equation}
    w_c = \frac{N}{28 \cdot N_c}
    \end{equation}

    where $N$ is the total number of samples and $N_c$ is the number of positive samples for class $c$. These weights are used in weighted loss functions or as sample weights during training.

    \subsection{Model 1: Simple LSTM Baseline}

    The first architecture serves as a baseline consisting of a standard unidirectional LSTM with dropout regularization.

    \subsubsection{Architecture}

    The model consists of:
    \begin{itemize}
        \item Embedding Layer: Maps token indices to 100-dimensional dense vectors, initialized randomly and trained end-to-end
        \item Dropout Layer: Spatial dropout with rate $p=0.2$
        \item LSTM Layer: 128 hidden units, processes sequence left-to-right, returns final hidden state
        \item Dropout Layer: Standard dropout with rate $p=0.5$ applied to LSTM output
        \item Output Layer: Fully connected layer with 28 units and sigmoid activation
    \end{itemize}

    The model computes:
    \begin{align}
    E &= \text{Embedding}(x) \in \mathbb{R}^{n \times 100} \\
    E' &= \text{Dropout}(E, p=0.2) \\
    h &= \text{LSTM}(E') \in \mathbb{R}^{128} \\
    h' &= \text{Dropout}(h, p=0.5) \\
    \hat{y} &= \sigma(W h' + b) \in [0,1]^{28}
    \end{align}

    where $\sigma$ is the sigmoid function and $W \in \mathbb{R}^{28 \times 128}$ are learned weights.

    \subsubsection{Training Configuration}

    Training uses binary cross-entropy with class weights, Adam optimizer with learning rate $\eta = 0.001$, batch size 64, 10 epochs, and prediction threshold $\tau = 0.3$ (lowered to improve recall for rare classes).

    \subsection{Model 2: BiLSTM with Attention}

    The second architecture enhances the baseline with bidirectional processing and an attention mechanism to focus on emotionally salient words.

    The BiLSTM processes sequences in both directions:
    \begin{align}
    \overrightarrow{h}_t &= \overrightarrow{\text{LSTM}}(e_t, \overrightarrow{h}_{t-1}) \\
    \overleftarrow{h}_t &= \overleftarrow{\text{LSTM}}(e_t, \overleftarrow{h}_{t+1}) \\
    h_t &= [\overrightarrow{h}_t; \overleftarrow{h}_t] \in \mathbb{R}^{256}
    \end{align}

    The attention layer implements additive (Bahdanau) attention:

    \begin{align}
    u_t &= \tanh(W_a h_t + b_a) \in \mathbb{R}^{256} \\
    \alpha_t &= \frac{\exp(u_t)}{\sum_{i=1}^n \exp(u_i)} \\
    c &= \sum_{t=1}^n \alpha_t h_t
    \end{align}

    where $c$ is the context vector representing the weighted aggregation of all time steps, and $\alpha_t$ are attention weights indicating the importance of each word.

    \subsubsection{Weighted Loss Function}

    We implement a custom weighted binary cross-entropy loss:

    \begin{equation}
    \mathcal{L} = -\frac{1}{N \cdot 28} \sum_{i=1}^N \sum_{c=1}^{28} w_c \left[ y_{i,c} \log(\hat{y}_{i,c}) + (1-y_{i,c}) \log(1-\hat{y}_{i,c}) \right]
    \end{equation}

    This loss function penalizes errors on rare emotions more heavily than common ones.

    \subsection{Model 3: CNN-BiLSTM Hybrid with Attention}

    The third architecture combines CNNs for local feature extraction with BiLSTMs for sequential modeling, integrating both spatial and temporal patterns.

    The model uses three parallel Conv1D layers with different kernel sizes (3, 4, 5), each with 64 filters. The parallel CNN layers capture n-gram patterns of different lengths:

    \begin{align}
    f^{(k)}_i &= \text{ReLU}(W^{(k)} \ast E_{i:i+k-1} + b^{(k)}) \\
    p^{(k)} &= \max(f^{(k)}_1, f^{(k)}_2, ..., f^{(k)}_{n-k+1})
    \end{align}

    where $k \in \{3, 4, 5\}$ represents kernel sizes, $\ast$ denotes convolution, and $p^{(k)}$ is the max-pooled feature map. The three feature maps are concatenated:

    \begin{equation}
    f_{CNN} = [p^{(3)}; p^{(4)}; p^{(5)}] \in \mathbb{R}^{192}
    \end{equation}

    Training uses weighted binary cross-entropy, Adam optimizer with $\eta = 0.001$, batch size 64, 15 epochs with early stopping (patience=3), and callbacks including ModelCheckpoint and ReduceLROnPlateau.

    \subsection{Model 4: BERT Fine-Tuning}

    The fourth architecture leverages transfer learning from pre-trained BERT (\texttt{bert-base-uncased}, 110M parameters with 12 transformer layers, 768 hidden dimensions, 12 attention heads) to exploit contextual word representations learned from massive corpora.

    We employ full fine-tuning with all BERT layers trainable, learning rate $2 \times 10^{-5}$ for BERT layers and $1 \times 10^{-4}$ for classification head, gradient accumulation over 2 steps (effective batch size 32), max sequence length 128 tokens, and 500 warmup steps with linear decay.

    The model processes text through BERT's self-attention mechanism:

    \begin{align}
    H &= \text{BERT}(x) \in \mathbb{R}^{n \times 768} \\
    h_{[CLS]} &= H_0 \in \mathbb{R}^{768} \\
    \hat{y} &= \sigma(W h_{[CLS]} + b) \in [0,1]^{28}
    \end{align}

    where $H_0$ is the representation of the [CLS] token, which BERT learns to encode sentence-level information.

    \section{Experimental Setup and Results}\label{sec:experimental}

    \subsection{Data Description}

    We use the GoEmotions dataset \cite{demszky2020goemotions}, the largest manually annotated dataset for fine-grained emotion classification from text, consisting of 58,009 Reddit comments from diverse subreddits with 28 emotion categories (27 emotions + neutral) in tab-separated format.

    \subsubsection{Data Acquisition and Annotation}

    The dataset was created by scraping publicly available Reddit comments from January 2005 to January 2019 across thousands of subreddits. Comments were filtered to be in English, contain 3-30 tokens, and exclude toxic content. The annotation process involved multiple raters per comment, with each rater selecting all applicable emotions from the 28-category taxonomy. The data is split into Training (43,410 samples, 74.9\%), Development (5,426 samples, 9.4\%), and Test (5,427 samples, 9.4\%) sets with stratified sampling to maintain label distribution. Inter-annotator agreement is measured at Krippendorff's $\alpha = 0.66$, indicating moderate to substantial agreement given the subjective nature of emotion annotation.

    \subsubsection{Emotion Categories}

    The 28 categories are: admiration, amusement, anger, annoyance, approval, caring, confusion, curiosity, desire, disappointment, disapproval, disgust, embarrassment, excitement, fear, gratitude, grief, joy, love, nervousness, optimism, pride, realization, relief, remorse, sadness, surprise, neutral.

    \subsubsection{Dataset Statistics and Characteristics}

    Table \ref{tab:dataset_stats} summarizes key dataset statistics.

    \begin{table}[htbp]
    \caption{GoEmotions Dataset Statistics}\label{tab:dataset_stats}
    \centering
    \begin{tabular}{@{}lr@{}}
    \toprule
    \textbf{Characteristic} & \textbf{Value} \\
    \midrule
    Total samples & 58,009 \\
    Training samples & 43,410 (74.9\%) \\
    Development samples & 5,426 (9.4\%) \\
    Test samples & 5,427 (9.4\%) \\
    \midrule
    Average text length (characters) & 68.2 \\
    Average text length (words) & 13.4 \\
    Vocabulary size (top 50K) & 49,837 \\
    \midrule
    Multi-label instances & 17,634 (30.4\%) \\
    Single-label instances & 40,375 (69.6\%) \\
    Average labels per instance & 1.43 \\
    Maximum labels per instance & 11 \\
    \midrule
    Most frequent emotion & Neutral (14,219) \\
    Least frequent emotion & Grief (77) \\
    Class imbalance ratio & 185:1 \\
    \bottomrule
    \end{tabular}
    \end{table}

    The dataset exhibits severe class imbalance with the most frequent emotion "Neutral" appearing 14,219 times (24.5\%) while the least frequent "Grief" appears only 77 times (0.13\%), resulting in an imbalance ratio of approximately 185:1. This imbalance significantly impacts model training and necessitates specialized techniques such as weighted loss functions and threshold optimization.

    \subsection{Evaluation Protocol}

    Due to the multi-label nature of the task, we employ multiple complementary metrics to comprehensively assess model performance from different perspectives.

    \subsubsection{Accuracy Metrics}

    We use standard classification accuracy defined as the percentage of correctly predicted instances. However, in multi-label settings, we also consider exact match accuracy (all labels must be correct) and subset accuracy (measures the fraction of samples for which the predicted label set exactly matches the true label set).

    \subsubsection{Hamming Loss}

    Hamming Loss measures the fraction of incorrectly predicted labels across all samples and all emotions:

    \begin{equation}
    \text{Hamming Loss} = \frac{1}{N \cdot L} \sum_{i=1}^N \sum_{j=1}^L \mathbb{1}(\hat{y}_{ij} \neq y_{ij})
    \end{equation}

    where $N$ is the number of samples, $L=28$ is the number of labels, and $\mathbb{1}$ is the indicator function. Lower values indicate better performance. This metric penalizes both false positives and false negatives equally.

    \subsubsection{Precision, Recall, and F1-Score}

    We compute precision, recall, and F1-score using both micro and macro averaging strategies:

    \textbf{Micro-averaging} treats all label-instance pairs equally, computing global true positives, false positives, and false negatives:

    \begin{align}
    \text{Precision}_{\text{micro}} &= \frac{\sum_{c=1}^{28} TP_c}{\sum_{c=1}^{28} (TP_c + FP_c)} \\
    \text{Recall}_{\text{micro}} &= \frac{\sum_{c=1}^{28} TP_c}{\sum_{c=1}^{28} (TP_c + FN_c)} \\
    \text{F1}_{\text{micro}} &= \frac{2 \cdot \text{Precision}_{\text{micro}} \cdot \text{Recall}_{\text{micro}}}{\text{Precision}_{\text{micro}} + \text{Recall}_{\text{micro}}}
    \end{align}

    \textbf{Macro-averaging} treats all emotion classes equally regardless of frequency, computing metrics per class and averaging:

    \begin{align}
    \text{Precision}_{\text{macro}} &= \frac{1}{28} \sum_{c=1}^{28} \frac{TP_c}{TP_c + FP_c} \\
    \text{Recall}_{\text{macro}} &= \frac{1}{28} \sum_{c=1}^{28} \frac{TP_c}{TP_c + FN_c} \\
    \text{F1}_{\text{macro}} &= \frac{1}{28} \sum_{c=1}^{28} \text{F1}_c
    \end{align}

    Macro-averaging gives equal weight to rare classes, making it particularly important for evaluating performance on underrepresented emotions.

    \subsubsection{ROC Curves and AUC-ROC}

    We compute the Receiver Operating Characteristic (ROC) curve for each emotion category by plotting the true positive rate (TPR) against the false positive rate (FPR) at various classification thresholds. The Area Under the ROC Curve (AUC-ROC) provides a single scalar value summarizing the model's discrimination ability:

    \begin{equation}
    \text{AUC-ROC}_{\text{macro}} = \frac{1}{28} \sum_{c=1}^{28} \text{AUC-ROC}_c
    \end{equation}

    This metric is threshold-independent and evaluates the model's ranking ability. An AUC-ROC of 0.5 indicates random performance, while 1.0 indicates perfect classification.

    \subsubsection{Precision-Recall Curves}

    For imbalanced datasets, precision-recall (PR) curves often provide more informative evaluation than ROC curves. We compute PR curves for each emotion and report the average precision (area under the PR curve) as an additional metric.

    \subsection{Comparative Methods}

    To evaluate the effectiveness of our proposed architectures, we establish comparisons with baseline methods of varying complexity:

    \subsubsection{Simple LSTM Baseline}

    A basic recurrent architecture serving as the simplest neural baseline. This model uses randomly initialized embeddings (dimension 100), a single LSTM layer (128 units), and binary cross-entropy loss with class weighting. The model uses a lower classification threshold ($\tau=0.3$) to improve recall on minority classes. This establishes the baseline performance for sequential neural models without pre-trained knowledge.

    \subsubsection{LSTM with Pre-trained Embeddings}

    To isolate the impact of pre-trained embeddings, we implement an LSTM variant using FastText word vectors . This model shares the same architecture as the simple LSTM but initializes the embedding layer with 300-dimensional FastText vectors trained on Common Crawl. The embeddings are fine-tuned during training. This baseline demonstrates whether pre-trained semantic knowledge significantly improves emotion classification.

    \subsubsection{Published BERT Baseline}

    Demszky et al. \cite{demszky2020goemotions} reported results using fine-tuned BERT-base-uncased on the GoEmotions dataset, achieving 46.0\% macro F1 and 64.5\% micro F1. We use these published results as a reference point for our BERT implementation to verify our fine-tuning approach and training procedures.

    \subsection{Experimental Protocol}

    \subsubsection{Hyperparameter Tuning}

    All hyperparameters are tuned using grid search on the development set to prevent overfitting to the test set. For LSTM-based models, we search over embedding dimensions $\{100, 128, 256\}$, LSTM units $\{64, 128, 256\}$, dropout rates $\{0.2, 0.3, 0.5\}$, and batch sizes $\{32, 64, 128\}$. For BERT, we tune learning rates $\{1 \times 10^{-5}, 2 \times 10^{-5}, 5 \times 10^{-5}\}$ and batch sizes $\{16, 32\}$. The development set is used exclusively for hyperparameter selection and early stopping decisions.

    \subsubsection{Training Configuration}

    All models are trained using the Adam optimizer with default beta parameters ($\beta_1=0.9$, $\beta_2=0.999$). We employ early stopping with patience of 3 epochs based on development set macro F1-score to prevent overfitting. Random seeds are fixed (seed=42) for all random number generators (NumPy, PyTorch, TensorFlow) to ensure reproducibility of results across runs.

    \subsubsection{Computational Infrastructure}

    Training is performed on NVIDIA RTX 3090 GPU with 24GB VRAM, CUDA 11.8, PyTorch 2.0, and TensorFlow 2.12. Approximate training times per full training run: LSTM (15 minutes), BiLSTM-Attention (25 minutes), CNN-BiLSTM (35 minutes), BERT (2.5 hours). All models converge within 10-15 epochs.

    \subsubsection{Threshold Selection}

    For multi-label classification, converting probability outputs to binary predictions requires selecting a threshold $\tau$. We evaluate thresholds in the range [0.1, 0.9] with step size 0.1 on the development set and select the threshold that maximizes macro F1-score. Different models may use different optimal thresholds based on their calibration properties.

    \subsubsection{Statistical Significance Testing}

    To verify that observed performance differences between models are statistically significant and not due to random variation, we apply McNemar's test for paired binary classifiers. We compute p-values for all pairwise model comparisons and report significance at $\alpha = 0.01$ level with Bonferroni correction for multiple comparisons.

    \subsection{Ablation Study}

    To understand the contribution of individual components to overall performance, we conduct systematic ablation experiments on Model 3 (CNN-BiLSTM-Attention), which represents the most complex architecture among our LSTM-based models.

    \subsubsection{Component Ablation}

    Table \ref{tab:ablation} shows performance when removing individual architectural components.

    \begin{table}[htbp]
    \caption{Component Ablation Results (Macro F1)}\label{tab:ablation}
    \begin{tabular}{@{}lcc@{}}
    \toprule
    \textbf{Configuration} & \textbf{Macro F1} & \textbf{$\Delta$} \\
    \midrule
    Full Model (CNN-BiLSTM-Attn) & 44.5\% & - \\
    \midrule
    - Remove Attention & 40.8\% & -3.7pp \\
    - Remove CNN Branch & 42.1\% & -2.4pp \\
    - Remove BiLSTM & 38.2\% & -6.3pp \\
    - Remove both CNN \& Attention & 36.9\% & -7.6pp \\
    \midrule
    CNN only (no LSTM) & 35.4\% & -9.1pp \\
    BiLSTM only (no CNN) & 42.1\% & -2.4pp \\
    \bottomrule
    \end{tabular}
    \end{table}

    BiLSTM is the most critical component with its removal causing the largest performance drop (-6.3pp), indicating sequential modeling is essential for capturing emotional context. Attention provides substantial gains (-3.7pp when removed), particularly important for rare emotions (+5.2pp for low-frequency emotions). CNN branch adds 2.4pp, suggesting multi-scale n-gram features complement BiLSTM's sequential modeling. Removing both CNN and attention (-7.6pp) is worse than sum of individual removals (-6.1pp), indicating synergistic effects.

    \subsubsection{Attention Mechanism Analysis}

    Visualization of attention weights reveals high attention on emotion-bearing words ("love", "hate", "amazing", "terrible"), attention to intensifiers ("very", "so", "really"), moderate attention to context words (negations, question words), and minimal attention to function words (articles, prepositions).

    \subsubsection{CNN Kernel Size Ablation}

    Multi-scale kernels \{3, 4, 5\} provide good balance between performance (44.5\% macro F1) and efficiency (3.4M parameters). Adding kernel size 6 provides marginal benefit (0.2pp) at cost of 9\% more parameters.

    \subsubsection{Embedding Strategies}

    Trainable random embeddings perform surprisingly well (44.5\%). Frozen pre-trained embeddings underperform, indicating domain mismatch. Fine-tuned FastText slightly outperforms random (44.7\% vs 44.5\%) but requires 23\% more training time. For this dataset, task-specific embeddings are more important than pre-trained general embeddings.

    \subsubsection{Dropout Regularization}

    Optimal dropout rate is 0.3, balancing regularization and model capacity. No dropout causes severe overfitting (26.9pp gap between train and test). Excessive dropout (0.7) underfits, preventing the model from learning complex patterns.

    \subsubsection{Weighted Loss Impact}

    Class-weighted binary cross-entropy achieves 44.5\% macro F1 compared to 38.7\% for standard BCE. Impact by frequency tier: high-frequency emotions show minimal difference (±0.5pp), mid-frequency emotions improve +3.2pp, and low-frequency emotions improve +8.7pp. Class weighting significantly improves rare emotion detection at minimal cost to frequent emotions.

    \subsection{Results and Discussion}

    \subsubsection{Overall Performance Comparison}

    Table \ref{tab:overall_results} summarizes the performance of all models on the test set using threshold $\tau=0.5$ except for Model 1 which uses $\tau=0.3$.

    \begin{table}[htbp]
    \caption{Overall Performance Comparison on Test Set}\label{tab:overall_results}
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
    \textbf{Model} & \textbf{Hamming} & \textbf{Micro} & \textbf{Macro} & \textbf{Micro} & \textbf{Micro} & \textbf{AUC-} \\
    & \textbf{Loss} $\downarrow$ & \textbf{F1} $\uparrow$ & \textbf{F1} $\uparrow$ & \textbf{Prec.} & \textbf{Rec.} & \textbf{ROC} $\uparrow$ \\
    \midrule
    LSTM* & 0.0417 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.4944 \\
    LSTM-FastText & 0.0390 & 0.2311 & 0.1843 & 0.6452 & 0.1408 & 0.7541 \\
    BiLSTM-Attn & 0.0373 & 0.3145 & 0.1546 & 0.6708 & 0.2054 & 0.8275 \\
    CNN-BiLSTM-Attn & 0.0370 & 0.3665 & 0.2535 & 0.6374 & 0.2572 & 0.8088 \\
    BERT & \textbf{0.0304} & \textbf{0.5519} & \textbf{0.4477} & \textbf{0.7139} & \textbf{0.4498} & \textbf{0.9274} \\
    \bottomrule
    \end{tabular}
    \end{table}

    \subsubsection{Key Findings}

    BERT achieves best overall performance with 55.2\% micro F1 and 44.8\% macro F1. The substantial gap between micro and macro F1 scores (10.4 percentage points) reflects the challenge of rare emotion classification. The baseline LSTM model completely failed (F1 = 0.0\%), predicting all zeros and requiring debugging. Pre-trained embeddings are essential: LSTM-FastText achieved 23.1\% micro F1 and 75.4\% AUC-ROC, demonstrating a 52.4 percentage point improvement in AUC-ROC over the broken baseline. Comparing LSTM-FastText to BiLSTM-Attention, we observe an 8.0 percentage point improvement in micro F1 (23.1\% → 31.5\%), demonstrating that bidirectional context and attention mechanisms help the model focus on emotionally salient words. Interestingly, BiLSTM-Attention (82.8\% AUC-ROC) outperforms CNN-BiLSTM-Attention (80.9\% AUC-ROC) despite the hybrid architecture's additional complexity, suggesting that the CNN component may introduce noise for this particular task.

    \subsubsection{Per-Emotion Performance Analysis}

    High-frequency emotions ($\geq$ 5,000 instances) achieve 70-90\% F1 scores. "Gratitude" achieves highest F1 (92.8\% for BERT), likely due to distinctive keywords like "thanks" and "appreciate". Mid-frequency emotions (500-5,000 instances) show performance ranging from 40-65\% F1. Low-frequency emotions ($\leq$  500 instances) suffer severe performance degradation: 11-39\% F1 across models. BERT shows largest gains for rare classes (e.g., grief: 27.8\% vs. 11.2\% for LSTM), with pre-trained knowledge helping generalize from few examples.

    \subsubsection{Confusion Matrix Analysis}

    Most frequently confused emotion pairs include: Annoyance/Anger (234 instances), Approval/Admiration (198), Excitement/Joy (167), Sadness/Disappointment (156), Love/Admiration (143), and Nervousness/Fear (112). These confusions are semantically reasonable as they represent closely related emotions with different intensities or similar valence.

    \subsubsection{Computational Efficiency}

    BERT provides best accuracy but is 32x larger and 10x slower than LSTM (110M vs 1.2M parameters, 12.4ms vs 1.2ms per sample). CNN-BiLSTM offers good balance: 7.1\% better macro F1 than LSTM with only 2.3x slower inference. For real-time applications, CNN-BiLSTM may be preferred over BERT.


    \section{Conclusion}\label{sec:conclusion}

    \subsection{Summary}

    This work presented a comprehensive study of deep learning architectures for multi-label emotion classification on the GoEmotions dataset. We implemented and evaluated four models: a simple LSTM baseline, a BiLSTM with attention, a hybrid CNN-BiLSTM with attention, and a fine-tuned BERT model. BERT achieves the best performance (55.2\% micro F1, 44.8\% macro F1), demonstrating the power of pre-trained transformers. The baseline LSTM completely failed (0.0\% F1), highlighting the critical importance of pre-trained embeddings—LSTM-FastText achieved 23.1\% micro F1, a dramatic improvement. Bidirectional context and attention mechanisms provide significant improvements (31.5\% micro F1 for BiLSTM-Attention vs 23.1\% for LSTM-FastText), particularly for rare emotions. Through extensive ablation studies, we quantified the contribution of different architectural components, with pre-trained embeddings and attention mechanisms proving most critical.

    \subsection{Limitations}
    \textbf{Limitations:} The following limitations were identified in our study:
    \begin{enumerate}
        \item GoEmotions consists solely of Reddit comments, which may not generalize to other domains.
        \item Emotion annotation is inherently subjective with moderate inter-annotator agreement ($\alpha=0.66$).
        \item Despite specialized techniques, rare emotions remain poorly classified.
        \item Maximum sequence length (70-128 tokens) may truncate important context.
        \item Models struggle with sarcastic or ironic expressions.
        \item BERT requires 10x more inference time than CNN-BiLSTM.
    \end{enumerate}

    \subsection{Future Directions}

    To address the limitations identified in our study, we propose several future directions for research:
    \begin{enumerate}
        \item Data augmentation techniques (back-translation, paraphrasing) to oversample rare emotions
        \item Few-shot learning approaches (MAML, Prototypical Networks) to improve learning from limited examples
        \item Hierarchical attention for word-level and sentence-level modeling
        \item Multi-task learning jointly with sentiment analysis and emotion intensity regression
        \item Newer transformers (RoBERTa, DeBERTa, domain-adapted BERT)
        \item Conversational context incorporation from Reddit threads
        \item Model compression (knowledge distillation, quantization) to deploy BERT-level performance with LSTM-level efficiency
        \item Cross-domain evaluation on Twitter, product reviews, news comments
        \end{enumerate}

    \subsection{Broader Impact}

    Emotion classification technology has significant societal implications. Positive applications include mental health monitoring, customer service automation, education feedback analysis, and market research. Ethical considerations include privacy concerns, bias amplification, misuse potential for manipulation, and the need for consent and transparency. Responsible deployment requires careful consideration of these factors and adherence to ethical AI principles.

    \backmatter

    \bmhead{Acknowledgements}

    We thank the creators of the GoEmotions dataset for making this research possible. We also acknowledge the open-source community for developing the tools and frameworks used in this work.

    \section*{Declarations}

    
    \textbf{Data availability:} The GoEmotions dataset is publicly available.

    \textbf{Code availability:} Implementation code and web application source code are found in the public repository \url{https://github.com/yos-r/go_emotions}.

    \textbf{Trained models} Trained models are available in \url{https://drive.google.com/drive/folders/13wW2r-JRe30SfZWQKqdIPJmo7HHtLSkD?usp=sharing}.

    
    \begin{thebibliography}{99}

    \bibitem{demszky2020goemotions}
    Demszky, D., Movshovitz-Attias, D., Ko, J., Cowen, A., Nemade, G., \& Ravi, S. (2020).
    GoEmotions: A dataset of fine-grained emotions.
    In \textit{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics} (pp. 4040--4054).

    \bibitem{devlin2019bert}
    Devlin, J., Chang, M. W., Lee, K., \& Toutanova, K. (2019).
    BERT: Pre-training of deep bidirectional transformers for language understanding.
    \textit{arXiv preprint arXiv:1810.04805}.

    \bibitem{vaswani2017attention}
    Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... \& Polosukhin, I. (2017).
    Attention is all you need.
    \textit{Advances in neural information processing systems}, 30.

    \bibitem{hochreiter1997long}
    Hochreiter, S., \& Schmidhuber, J. (1997).
    Long short-term memory.
    \textit{Neural computation}, 9(8), 1735--1780.

    \bibitem{kim2014convolutional}
    Kim, Y. (2014).
    Convolutional neural networks for sentence classification.
    \textit{arXiv preprint arXiv:1408.5882}.

    \bibitem{bahdanau2014neural}
    Bahdanau, D., Cho, K., \& Bengio, Y. (2015).
    Neural machine translation by jointly learning to align and translate.
    In \textit{International Conference on Learning Representations}.

    \bibitem{mohammad2013nrc}
    Mohammad, S. M., Kiritchenko, S., \& Zhu, X. (2013).
    NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.
    In \textit{Second Joint Conference on Lexical and Computational Semantics}.

    \bibitem{ekman1992argument}
    Ekman, P. (1992).
    An argument for basic emotions.
    \textit{Cognition \& emotion}, 6(3-4), 169--200.

    \bibitem{plutchik2001nature}
    Plutchik, R. (2001).
    The nature of emotions: Human emotions have deep evolutionary roots.
    \textit{American scientist}, 89(4), 344--350.

    \bibitem{alm2005emotions}
    Alm, C. O., Roth, D., \& Sproat, R. (2005).
    Emotions from text: machine learning for text-based emotion prediction.
    In \textit{Proceedings of human language technology conference and conference on empirical methods in natural language processing} (pp. 579--586).

    \bibitem{cho2014learning}
    Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., \& Bengio, Y. (2014).
    Learning phrase representations using RNN encoder-decoder for statistical machine translation.
    \textit{arXiv preprint arXiv:1406.1078}.

    \bibitem{schuster1997bidirectional}
    Schuster, M., \& Paliwal, K. K. (1997).
    Bidirectional recurrent neural networks.
    \textit{IEEE transactions on Signal Processing}, 45(11), 2673--2681.

    \bibitem{zhou2016attention}
    Zhou, P., Shi, W., Tian, J., Qi, Z., Li, B., Hao, H., \& Xu, B. (2016).
    Attention-based bidirectional long short-term memory networks for relation classification.
    In \textit{Proceedings of the 54th annual meeting of the association for computational linguistics} (pp. 207--212).

    \bibitem{yang2016hierarchical}
    Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., \& Hovy, E. (2016).
    Hierarchical attention networks for document classification.
    In \textit{Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies} (pp. 1480--1489).

    \bibitem{radford2018improving}
    Radford, A., Narasimhan, K., Salimans, T., \& Sutskever, I. (2018).
    Improving language understanding by generative pre-training.

    \bibitem{read2011classifier}
    Read, J., Pfahringer, B., Holmes, G., \& Frank, E. (2011).
    Classifier chains for multi-label classification.
    \textit{Machine learning and knowledge discovery in databases: European conference, ECML PKDD 2009}, 254--269.

    \bibitem{nam2014large}
    Nam, J., Kim, J., Mencía, E. L., Gurevych, I., \& Fürnkranz, J. (2014).
    Large-scale multi-label text classification—revisiting neural networks.
    In \textit{Joint European conference on machine learning and knowledge discovery in databases} (pp. 437--452).

    \bibitem{yang2018sgm}
    Yang, P., Sun, X., Li, W., Ma, S., Wu, W., \& Wang, H. (2018).
    SGM: Sequence generation model for multi-label classification.
    In \textit{Proceedings of the 27th International Conference on Computational Linguistics} (pp. 3915--3926).

    \bibitem{liu2017deep}
    Liu, J., Chang, W. C., Wu, Y., \& Yang, Y. (2017).
    Deep learning for extreme multi-label text classification.
    In \textit{Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval} (pp. 115--124).

    \bibitem{chawla2002smote}
    Chawla, N. V., Bowyer, K. W., Hall, L. O., \& Kegelmeyer, W. P. (2002).
    SMOTE: synthetic minority over-sampling technique.
    \textit{Journal of artificial intelligence research}, 16, 321--357.

    \bibitem{lin2017focal}
    Lin, T. Y., Goyal, P., Girshick, R., He, K., \& Dollár, P. (2017).
    Focal loss for dense object detection.
    In \textit{Proceedings of the IEEE international conference on computer vision} (pp. 2980--2988).

    \bibitem{wu2020multi}
    Wu, T., Huang, Q., Liu, Z., Wang, Y., \& Lin, D. (2020).
    Distribution-balanced loss for multi-label classification in long-tailed datasets.
    \textit{arXiv preprint arXiv:2007.09654}.

    \bibitem{zhang2021distribution}
    Zhang, S., Li, Z., Yan, S., He, X., \& Sun, J. (2021).
    Distribution alignment: A unified framework for long-tail visual recognition.
    In \textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition} (pp. 2361--2370).

    \bibitem{mohammad2018semeval}
    Mohammad, S., Bravo-Marquez, F., Salameh, M., \& Kiritchenko, S. (2018).
    SemEval-2018 Task 1: Affect in tweets.
    In \textit{Proceedings of the 12th international workshop on semantic evaluation} (pp. 1--17).

    \bibitem{chatterjee2019semeval}
    Chatterjee, A., Narahari, K. N., Joshi, M., \& Agrawal, P. (2019).
    SemEval-2019 task 3: EmoContext contextual emotion detection in text.
    In \textit{Proceedings of the 13th international workshop on semantic evaluation} (pp. 39--48).

    \bibitem{acheampong2021transformer}
    Acheampong, F. A., Chen, W., \& Nunoo-Mensah, H. (2021).
    Transformer models for text-based emotion detection: a review of BERT-based approaches.
    \textit{Artificial Intelligence Review}, 54(8), 5789--5829.

    \bibitem{zhou2015cnn}
    Zhou, C., Sun, C., Liu, Z., \& Lau, F. (2015).
    A C-LSTM neural network for text classification.
    \textit{arXiv preprint arXiv:1511.08630}.

    \bibitem{wang2016combination}
    Wang, X., Jiang, W., \& Luo, Z. (2016).
    Combination of convolutional and recurrent neural network for sentiment analysis of short texts.
    In \textit{Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers} (pp. 2428--2437).

    \bibitem{yadav2020hybrid}
    Yadav, A., \& Vishwakarma, D. K. (2020).
    Sentiment analysis using deep learning architectures: a review.
    \textit{Artificial Intelligence Review}, 53(6), 4335--4385.

    \end{thebibliography}

    \end{document}
    